\documentclass[12pt,a4paper]{article}
\usepackage[margin=2cm]{geometry}
\usepackage{amsmath,amsfonts,amsthm}
\newtheorem{example}{Example}
\usepackage{graphicx}
\usepackage[backend=biber]{biblatex}
\usepackage{cleveref}
\usepackage{kbordermatrix}
\bibliography{MLnotes}
\title{Notes on the Mittag--Leffler function}
\author{William McLean}
\date{\today}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\DeclareMathOperator*{\res}{res}
\newcommand{\arcosh}{\operatorname{arcosh}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% % %
\begin{document}
\maketitle
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
The two-argument, Mittag--Leffler function is defined by the power series
\begin{equation}\label{eq: E alpha beta def}
E_{\alpha,\beta}(z)=\sum_{n=0}^\infty\frac{z^n}{\Gamma(\beta+n\alpha)}
\quad\text{for $z\in\mathbb{C}$, $\alpha\ge0$, $\beta\in\mathbb{R}$.}
\end{equation}
The reciprocal of the Gamma function may be written as an integral along a
Hankel contour,
\[
\frac{1}{\Gamma(\beta+n\alpha)}=\frac{1}{2\pi i}\int_{-\infty}^{0^+}
    \frac{e^w\,dw}{w^{\beta+n\alpha}},
\]
where we take a branch cut along the negative real axis so that 
$-\pi<\arg w<\pi$. Therefore, if $|z|<|w^\alpha|$ for all~$w$ on the contour, 
then
\[
E_{\alpha,\beta}(z)=\frac{1}{2\pi i}\int_{-\infty}^{0^+}\frac{e^w}{w^\beta}
    \sum_{n=0}^\infty(zw^{-\alpha})^n\,dw
    =\frac{1}{2\pi i}\int_{-\infty}^{0^+}\frac{e^w}{w^\beta}\,
    \frac{dw}{1-zw^{-\alpha}},
\]
and so
\begin{equation}\label{eq: integral repn}
E_{\alpha,\beta}(z)=\frac{1}{2\pi i}\int_{-\infty}^{0^+}
    \frac{e^w\,dw}{w^\beta-zw^{\beta-\alpha}}.
\end{equation}

If $\alpha>0$, then it suffices to treat the case~$\beta>0$ because the identity
\begin{equation}\label{eq: beta identity}
z^rE_{\alpha,\beta+r\alpha}=E_{\alpha,\beta}(z)
    -\sum_{n=0}^{r-1}\frac{z^n}{\Gamma(\beta+n\alpha)},\quad 
r\in\{1,2,3,\ldots\},
\end{equation}
allows us to express $E_{\alpha,\beta}(z)$ in terms 
of~$E_{\alpha,\beta+r\alpha}(z)$.  Also, in the case~$\alpha=0$,
\[
E_{0,\beta}(z)=\frac{1}{\Gamma(\beta)(1+z)}.
\]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Taylor approximation for $z$ near zero}
Since 
$\Gamma(t)\ge\Gamma(1{\cdot}46163\,21456\,31155)=0{\cdot}88560\,31944\,10889
=1/1{\cdot}12917\,38854\,50141$, a crude bound for the remainder term~$R_N(z)$ 
in the Taylor expansion,
\[
E_{\alpha,\beta}(z)=\sum_{n=0}^N\frac{z^n}{\Gamma(\beta+n\alpha)}+R_N(z)
\]
is given by
\[
|R_N(z)|\le1{\cdot}3\sum_{n=N+1}^\infty|z|^n=\frac{1{\cdot}3\,|z|^{N+1}}{1-|z|}
\quad\text{for $|z|<1$.}
\]
Hence, if
\[
N+1\ge\frac{\log\bigl((1-|z|)\epsilon/1{\cdot}3\bigr)}{\log|z|}
\quad\text{and}\quad|z|<1,
\]
then $|R_N(z)|\le\epsilon$.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Quadrature approximation: negative real argument}\label{sec: quad neg}
Assume now that $0<\alpha<1$. Let $x>0$ and put $z=-x$ 
in~\eqref{eq: integral repn} to obtain
\[
E_{\alpha,\beta}(-x)=\frac{1}{2\pi i}\int_{-\infty}^{0^+}
    e^wF(w,x)\,dw
\quad\text{where}\quad
F(w,x)=\frac{w^{\alpha-\beta}}{w^\alpha+x}.
\]
We note that since $|\arg(w)|<\pi$ if follows that $|\arg(w^\alpha)|<\alpha\pi$
so $w^\alpha+x\ne0$.  Consider the Hankel contour parameterised 
by~\cite{WeidemanTrefethen2007}
\[
w(u)=\mu\bigl(1+\sin(iu-\phi)\bigr)\quad\text{for $-\infty<u<\infty$,}
\]
with $\mu>0$ and $0<\phi<\pi/2$. Since
\begin{align*}
\Re w&=\mu(1-\cosh u\,\sin\phi),\\
\Im w&=\mu\sinh u\,\cos\phi,
\end{align*}
we have
\[
\biggl(\frac{\Re w-1}{\mu\sin\phi}\biggr)^2
    -\biggl(\frac{\Im w}{\mu\cos\phi}\biggr)^2=1,
\]
showing that the contour is the left branch of an hyperbola with asymptotes
\[
\Im w=\pm(\Re w-1)\tan\phi.
\]
Thus,
\[
E_{\alpha,\beta}(-x)=\frac{1}{2\pi i}\int_{-\infty}^\infty 
e^{w(u)}F\bigl(w(u),x\bigr)z'(u)\,du\approx Q_h(x),
\]
where, for a step size~$h>0$, we put
\[
u_n=nh,\qquad w_n=w(u_n),\qquad w'_n=w'(u_n),
\]
and define the series approximation
\[
Q_h(x)=\frac{h}{2\pi i}\sum_{n=-\infty}^\infty e^{w_n}F(w_n,x)w'_n.
\]
For a fixed~$v$ with $0<\phi+v<\pi/2$, we see that
$w(u+iv)=\mu\bigl[1+\sin(iu-(\phi+v)\bigr)\bigr]$ parameterises the left 
branch of an hyperbola with asymptotes
\[
\Im w=\pm(\Re w-1)\tan(\phi+v).
\]
Putting
\[
M(x,v)=\int_{-\infty}^\infty\bigl|\exp\bigl(w(u+iv)\bigr)
    F\bigl(w(u+iv),x\bigr)w'(u+iv)\bigr|\,du
    \quad\text{for $-\phi<v<\frac{\pi}{2}$,}
\]
we have the error bound~\cite[Theorem~2.1]{WeidemanTrefethen2007}
\[
|Q_h(x)-E_{\alpha,\beta}(-x)|\le\frac{M(x,r)}{\exp(2\pi r/h)-1}
    +\frac{M(x,-s)}{\exp(2\pi s/h)-1}
\]
for $0<s<\phi$ and $0<r<\pi/2-\phi$.  For the finite sum,
\begin{equation}\label{eq: Q_hN}
Q_{h,N}(x)=\frac{h}{2\pi i}\sum_{n=-N}^N e^{w_n}F(w_n,x)w'_n,
\end{equation}
we have an additional truncation error
\[
T_{h,N}=h\sum_{n=N+1}^\infty\bigl(e^{w_n}F(w_n,x)w'_n
    +e^{w_{-n}}F(w_{-n},x)w'_{-n}\bigr).
\]
Choosing the largest possible values $s=\phi$~and $r=\pi/2-\phi$, the three
error terms are of order $\exp\bigl(-(\pi^2-2\pi\phi)/h\bigr)$,
$\exp(\mu-2\pi\phi/h)$ and $\exp\bigl(\mu(1-\cosh(Nh)\sin\phi)\bigr)$.
To balance these quantities, we seek to choose the parameters so that
\cite[(4.2)]{WeidemanTrefethen2007}
\[
-\frac{\pi^2-2\pi\phi}{h}=\mu-\frac{2\pi\phi}{h}
    =\mu\bigl(1-\cosh(Nh)\sin\phi\bigr).
\]
The left-hand equation implies that
\[
\mu=\frac{4\pi\phi-\pi^2}{h},
\]
which allows us to eliminate $\mu$ in the right-hand equation, to obtain
\[
\cosh(Nh)=\frac{2\pi\phi}{(4\phi-\pi)\pi\sin\phi}.
\]
We therefore define
\[
a(\phi)=\arcosh\biggl(\frac{2\pi\phi}{(4\phi-\pi)\pi\sin\phi}\biggr)
\quad\text{and}\quad b(\phi)=\frac{4\pi\phi-\pi^2}{A(\phi)}
\]
for $\pi/4<\phi<\pi/2$, and set
\[
h=\frac{a(\phi)}{N}\quad\text{and}\quad \mu=b(\phi)\,N,
\]
so that the error~$Q_{h,N}(x)-E_{\alpha,\beta}(-x)$ is order~$e^{-C(\phi)N}$ 
for
\begin{equation}\label{eq: C function}
C(\phi)=\frac{\pi^2-2\pi\phi}{A(\phi)}.
\end{equation}
\Cref{fig: C plot} shows a plot of~$C(\phi)$, and we find using a standard 
optimization package that the maximum value occurs at
\[
\phi=\phi_\star=1.172104\quad\text{with}\quad C(\phi_\star)=2.315654.
\]
Furthermore,
\[
a(\phi_\star)=1.081792\quad\text{and}\quad
b(\phi_\star)=4.492075,
\]
leading to the optimal values 
\[
h=h_\star=\frac{a(\phi_\star)}{N}\quad\text{and}\quad
\mu=\mu_\star=b(\phi_\star)\,N.
\]
Since $e^{B(\phi_\star)}=10.131547$ we expect to gain about an extra decimal
digit of accuracy each time we increase $N$ by~$1$.

\begin{figure}
\caption{Plot of $C(\phi)$ from~\eqref{eq: C function} showing the 
maximum at~$\phi_\star$.}\label{fig: C plot}
\begin{center}
\includegraphics[scale=0.75]{Bplot.pdf}
\end{center}
\end{figure}

Notice that
\[
\frac{hw'(u)}{2\pi i}=\frac{h\mu}{2\pi}\,\cos(iu-\phi)
    =\frac{a(\phi_\star)b(\phi_\star)}{2\pi}\,\cos(iu-\phi)
\]
so we can re-write the sum~\eqref{eq: Q_hN} as
\[
Q_{hN}(x)=\frac{a(\phi_\star)b(\phi_\star)}{2\pi}\sum_{n=-N}^N
    e^{w_n}F(w_n,x)\cos(iu_n-\phi).
\]
Also, $u_{-n}=-u_n$, $w(-u)=\overline{w(u)}$~and 
$F(\bar w,x)=\overline{F(w,x)}$, so the terms of the sum occur in complex 
conjugate pairs,
\[
e^{w_{-n}}F(w_{-n},x)\cos(iu_{-n}-\phi)
    =\overline{e^{w_n}F(w_n,x)\cos(iu_n-\phi)}.
\]
Thus,
\[
Q_{h,N}(x)=\frac{a(\phi_\star)b(\phi_\star)}{\pi}\biggl(
    \tfrac12e^{w_0}F(w_0,x)\cos\phi
    +\sum_{n=1}^N\Re\bigl(e^{w_n}F(w_n,x)\cos(iu_n-\phi)\bigr)\biggr),
\]
but note that $w_n$~and $u_n$ depend on~$N$ through $h$~and $\mu$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Quadrature approximation: positive real argument}
We now consider
\[
E_{\alpha,\beta}(x)=\frac{1}{2\pi i}\int_{-\infty}^{0^+}
    e^wF(w,-x)\,dx,
\]
where the Hankel contour must pass to the right of the pole at~$w=x^{1/\alpha}$.
Write
\[
F(w,-x)=w^{\alpha-\beta}\,\frac{G(w,x)}{w-x^{1/\alpha}}\quad\text{where}\quad
G(w,x)=\frac{w-x^{1/\alpha}}{w^\alpha-x},
\]
and observe that $G(w,x)$ has a removable singularity at~$w=x^{1/\alpha}$. In 
fact, using L'H\^ospital's rule,
\[
G(x^{1/\alpha},x)=\lim_{w\to x^{1/\alpha}}
    \frac{w-x}{w^\alpha-x}=\frac{x^{(1/\alpha)-1}}{\alpha},
\]
and we have
\[
E_{\alpha,\beta}(x)=\frac{1}{2\pi i}\int_{-\infty}^{0^+}e^w w^{\alpha-\beta}\,
    \frac{G(w,x)}{w-x^{1/\alpha}}\,dw.
\]
For $w\ne x^{1/\alpha}$, let
\[
H(w,x)=\frac{w^{\alpha-\beta}G(w,x)-x^{1-\beta/\alpha}G(x^{1/\alpha},x)}%
{w-x^{1/\alpha}}
=\frac{w^{\alpha-\beta}}{w^\alpha-x}
    -\frac{x^{(1-\beta)/\alpha}}{\alpha(w-x^{1/\alpha})},
\]
so that
\[
E_{\alpha,\beta}(-x)=\frac{x^{1-\beta/\alpha}G(x^{1/\alpha},x)}{2\pi i}
\int_{\mathcal{C}}\frac{e^w}{w-x^{1/\alpha}}\,dw
    +\frac{1}{2\pi i}\int_{\mathcal{C}}e^wH(w,x)\,dw
\]
and thus
\begin{equation}\label{eq: pos H}
E_{\alpha,\beta}(x)=\alpha^{-1}x^{(1-\beta)/\alpha}\exp(x^{1/\alpha})
    +\frac{1}{2\pi i}\int_{-\infty}^{0^+}e^w H(w,x)\,dw.
\end{equation}
The integral term in \eqref{eq: pos H} can be approximated using the same 
approach as in \Cref{sec: quad neg}, with $H(w,x)$ replacing $F(w,x)$.

To evaluate $H(w,x)$ for $w$ near~$x^{1/\alpha}$, put
\[
w=x^{1/\alpha}(1+\epsilon)\quad\text{where}\quad 
\epsilon=\frac{w-x^{1/\alpha}}{x^{1/\alpha}},
\]
and define
\[
\psi_{1,\alpha}(\epsilon)=\frac{(1+\epsilon)^\alpha-1}{\epsilon}
\quad\text{and}\quad
\psi_{2,\alpha}(\epsilon)
=\frac{(1+\epsilon)^\alpha-(1+\alpha\epsilon)}{\epsilon^2}
\]
so that
\[
(1+\epsilon)^\alpha=1+\epsilon\psi_{1,\alpha}(\epsilon)=1+\alpha\epsilon
    +\epsilon^2\psi_{2,\alpha}(\epsilon).
\]
Note that
\[
\psi_{1,\alpha}(\epsilon)=\sum_{n=0}^\infty c_{n+1}(\alpha)\epsilon^n
\quad\text{and}\quad
\psi_{2,\alpha}(\epsilon)=\sum_{n=0}^\infty c_{n+2}(\alpha)\epsilon^n
\quad\text{for $|\epsilon|<1$,}
\]
where
\[
c_n(\alpha)=\frac{\alpha}{1}\cdot\frac{\alpha-1}{2}\cdots\frac{\alpha-n+1}{n},
\]
or in other words,
\[
c_1(\alpha)=\alpha\qquad\text{and}\qquad 
c_n(\alpha)=-\frac{n-\alpha-1}{n}\,c_{n-1}(\alpha)
\quad\text{for $n\ge2$.}
\]
In this way,
\[
w^\alpha-x=x[(1+\epsilon)^\alpha-1]=x\epsilon\psi_{1,\alpha}(\epsilon)
    =x[\alpha\epsilon+\epsilon^2\psi_{2,\alpha}(\epsilon)].
\]
Write
\[
H(w,x)=w^{\alpha-\beta}\,\frac{G(w,x)-G(x^{1/\alpha},x)}{w-x^{1/\alpha}}
+\frac{w^{\alpha-\beta}-(x^{1/\alpha})^{\beta-\alpha}}{w-x^{1/\alpha}}
    \,G(x^{1/\alpha},x),
\]
and observe that
\[
\frac{G(w,x)-G(x^{1/\alpha},x)}{w-x^{1/\alpha}}
=\frac{1}{w^\alpha-x}-\frac{\alpha^{-1}x^{1/\alpha}}{w-x^{1/\alpha}}
=\frac{(w-x^{1/\alpha})-\alpha^{-1}x^{(1/\alpha)-1} (w^\alpha-x)}%
{(w^\alpha-x)(w-x^{1/\alpha})}.
\]
The numerator equals
\[
\epsilon x^{1/\alpha}-\alpha^{-1}x^{1/\alpha}\bigl[\alpha\epsilon
    +\epsilon^2\psi_{2,\alpha}(\epsilon)\bigr]
    =-\frac{x^{1/\alpha}}{\alpha}\,\psi_{2,\alpha}(\epsilon)
\]
and the denominator equals
\[
\bigl[x\epsilon\psi_{1,\alpha}(\epsilon)\bigr]\epsilon x^{1/\alpha}
    =x^{1+1/\alpha}\epsilon^2\psi_{2,\alpha}(\epsilon),
\]
so
\[
\frac{G(w,x)-G(x^{1/\alpha},x)}{w-x^{1/\alpha}}
    =-\frac{\psi_{2,\alpha}(\epsilon)}{\alpha x\psi_{1,\alpha}(\epsilon)}
    \quad\text{for $|w-x^{1/\alpha}|<x^{1/\alpha}$.}
\]
Similarly,
\[
w^{\alpha-\beta}-(x^{1/\alpha})^{\alpha-\beta}
    =(x^{1/\alpha})^{\alpha-\beta}[(1+\epsilon)^{\alpha-\beta}-1]
    =x^{1-\beta/\alpha}\epsilon\psi_{1,\alpha-\beta}(\epsilon)
\]
and hence
\[
\frac{w^{\alpha-\beta}-(x^{1/\alpha})^{\beta-\alpha}}{w-x^{1/\alpha}}
    =x^{1-(1+\beta)/\alpha}\psi_{1,\alpha-\beta}(\epsilon)
    \quad\text{for $|w-x^{1/\alpha}|<x^{1/\alpha}$.}
\]
Thus,
\begin{align*}
H(w,x)&=\alpha^{-1}x^{-\beta/\alpha}\psi_{1,\alpha-\beta}(\epsilon)
-w^{\alpha-\beta}\frac{\psi_{2,\alpha}(\epsilon)}%
{\alpha x\psi_{1,\alpha}(\epsilon)}\\
    &=\frac{x^{-\beta/\alpha}}{\alpha}\biggl(\psi_{1,\alpha-\beta}(\epsilon)
    -(1+\epsilon)^{\alpha-\beta}\,\frac{\psi_{2,\alpha}(\epsilon)}%
{\psi_{1,\alpha}(\epsilon)}\biggr),
\end{align*}
and in particular, since
\[
\psi_{1,\alpha}(0)=c_1(\alpha)=\alpha
\quad\text{and}\quad
\psi_{2,\alpha}(0)=c_2(\alpha)=\frac{\alpha(\alpha-1)}{2},
\]
we see that
\[
H(x^{1/\alpha},x)=\frac{x^{-\beta/\alpha}}{\alpha}\biggl((\alpha-\beta)
    -\frac{\alpha-1}{2}\biggr)=\frac{1+\alpha-2\beta}{2\alpha x^{\beta/\alpha}}.
\]

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Asymptotic expansion: negative real argument}
Since
\[
\frac{1}{w^\beta-zw^{\beta-\alpha}}=\frac{-1}{zw^{\beta-\alpha}}
    \,\frac{1}{1-w^\alpha z^{-1}}
\]
and
\[
\frac{1}{1-w^\alpha z^{-1}}=\sum_{n=0}^{N-1}(w^\alpha z^{-1})^n
    +\frac{(w^\alpha z^{-1})^N}{1-w^\alpha z^{-1}},
\]
we see from~\eqref{eq: integral repn} that
\[
E_{\alpha,\beta}(z)=\sum_{n=0}^{N-1}\frac{-1}{2\pi i}\int_{-\infty}^{0^+}
    \frac{e^w(w^\alpha z^{-1})^n}{zw^{\beta-\alpha}}\,dw+R_N(z),
\]
where the remainder term is
\[
R_N(z)=\frac{-1}{2\pi i}\int_{-\infty}^{0^+}
    \frac{e^w(w^\alpha z^{-1})^N\,dw}{zw^{\beta-\alpha}(1-w^\alpha z^{-1})}.
\]
The $n$th term equals
\[
\frac{-z^{-1-n}}{2\pi i}\int_{-\infty}^{0^+}e^w w^{(n+1)\alpha-\beta}\,dw,
\]
and if we assume $\alpha-\beta>-1$ then by collapsing the contour onto the 
negative real axis and using the substitutions~$w=re^{\pm i\pi}$, we obtain
\[
\frac{-1}{2\pi i}\int_{-\infty}^{0^+}e^w w^{(n+1)\alpha-\beta}\,dw
    =\frac{e^{i\pi[(n+1)\alpha-\beta]}-e^{-i\pi[(n+1)\alpha-\beta]}}{2\pi i}
    \int_0^\infty e^{-r}r^{(n+1)\alpha-\beta}\,dr
\]
so
\[
E_{\alpha,\beta}(z)=R_N(z)+\frac{1}{\pi}\sum_{n=0}^{N-1}
    \sin\pi[(n+1)\alpha-\beta]\,\Gamma\bigl((n+1)\alpha-\beta+1\bigr)z^{-n-1}.
\]
Also,
\begin{equation}\label{eq: RN(z)}
R_N(z)=\frac{-z^{-N-1}}{2\pi i}\int_{-\infty}^{0^+}
    \frac{e^w w^{(N+1)\alpha-\beta}}{1-z^{-1}w^\alpha}\,dw.
\end{equation}

Suppose $z=-x<0$, and choose $\theta\in(0,\pi/2)$.  Define the semi-infinite
lines
\[
\varGamma_\pm=\{\,re^{\pm i(\pi-\theta)}:0<r<\infty\,\},
\]
and consider
\[
R_N(-x)=\frac{(-1)^Nx^{-N-1}}{2\pi i}\int_{-\infty}^{0+}
    \frac{e^w w^{(N+1)\alpha-\beta}}{1+x^{-1}w^\alpha}\,dw.
\]
By integrating along $-\varGamma_-$ and $\varGamma_+$, we see that
\[
|R_N(-x)|\le\frac{x^{-N-1}}{\pi}\int_0^\infty
\frac{e^{-r\cos\theta}r^{(N+1)\alpha-\beta}}%
{|1+x^{-1}r^\alpha e^{i\phi}|}\,dr\quad\text{where}\quad
\phi=(\pi-\theta)\alpha.
\]
Since
\[
|1+x^{-1}r^\alpha e^{i\phi}|\ge\Re(1+x^{-1}r^\alpha\cos\phi)
    =1+x^{-1}r^\alpha\cos\phi,
\]
if $0<\phi\le\pi/2$ then $\cos\phi\ge0$ and hence $|1+x^{-1}r^\alpha|\ge1$ so,
using the substitution~$y=r\cos\theta$,
\begin{align*}
|R_N(-x)|&\le\frac{x^{-N-1}}{\pi}\int_0^\infty 
    e^{-r\cos\theta}r^{(N+1)\alpha-\beta}\,dr
    =\frac{x^{-N-1}}{\pi}\,\frac{1}{(\cos\theta)^{(N+1)\alpha-\beta+1}}
    \int_0^\infty e^{-y}y^{(N+1)\alpha-\beta}\,dy\\
    &=\frac{(\cos\theta)^{1-\beta}}{\pi}\,
    \bigl(x(\cos\theta)^\alpha\bigr)^{-N-1}\,
    \Gamma\bigl((N+1)\alpha-\beta+1\bigr)=O(x^{-N-1}).
\end{align*}
Otherwise, if $\pi/2<\phi<\pi$, then $\cos\phi<0$ and we put $q=x/|\cos\phi|$ 
so that
\[
|1+x^{-1}r^\alpha e^{i\phi}|\ge1/2\quad\text{for}\quad 0<r<(q/2)^{1/\alpha}
\quad\text{and}\quad (3q/2)^{1/\alpha}<r<\infty,
\]
whereas, for $(q/2)^{1/\alpha}\le r\le(3q/2)^{1/\alpha}$, we use the lower bound
\[
|1+x^{-1}r^\alpha e^{i\phi}|\ge|\Im(1+x^{-1}r^\alpha e^{i\phi})|
    =x^{-1}r^\alpha|\sin\phi|.
\]
Thus, $|R_N(-x)|\le I_1+I_2$ where $I_1=O(x^{-N-1})$ and
\begin{align*}
I_2&=\frac{x^{-N}}{\pi\sin\phi}\int_{(q/2)^{1/\alpha}}^{(3q/2)^{1/\alpha}}
    e^{-r\cos\theta}r^{N\alpha-\beta}\,dr\\
&\le\frac{x^{-N}}{\pi\sin\phi}\,\exp\bigl(-(q/2)^{1/\alpha}\cos\theta\bigr)
    \biggl[\frac{r^{N\alpha-\beta+1}}{N\alpha-\beta+1}
    \biggr]_{(q/2)^{1/\alpha}}^{(3q/2)^{1/\alpha}}\\
&\le\frac{x^{-N-1}q\cos\phi}{\pi\sin\phi}\,(3q/2)^{N+(1-\beta)/\alpha}\,
\frac{\exp\bigl(-(q/2)^{1/\alpha}\cos\theta\bigr)}{N\alpha-\beta+1}
=O(x^{-N-1}),
\end{align*}
yielding the asymptotic expansion
\[
E_{\alpha,\beta}(-x)=\frac{1}{\pi}\sum_{n=1}^N(-1)^n
    \sin\pi(n\alpha-\beta)\,\Gamma(n\alpha-\beta+1)\,x^{-n}
    +O(x^{-N-1})\quad\text{as $x\to\infty$.}
\]
Notice that the identity
\[
\Gamma(z)\,\Gamma(1-z)=\frac{\pi}{\sin\pi z}
\]
allows us to re-write the expansion as
\[
E_{\alpha,\beta}(-x)=-\sum_{n=1}^N\frac{(-x)^n}{\Gamma(\beta-n\alpha)}
    +O(x^{-N-1}).
\]

By Stirling's formula,
\[
\log\Gamma(z)=z\log z-z+\tfrac12\log z+O(1)\quad\text{as $|z|\to\infty$,}
\]
so as $n\to\infty$, and putting $\beta'=1-\beta$,
\begin{align*}
\smash[b]{\log\biggl(\frac{\Gamma(n\alpha+\beta')}%
{\Gamma\bigl((n-1)\alpha+\beta')}\biggr)}&=
\bigl(n\alpha+\beta'\bigr)\log(n\alpha+\beta')\\
&\qquad{}-\bigl((n-1)\alpha+\beta'\bigr)
    \log\bigl((n-1)\alpha+\beta'\bigr)+O(1)\\
    &=\alpha\log(n\alpha+\beta')+\bigl((n-1)\alpha+\beta'\bigr)
    \log\frac{n\alpha+\beta'}{(n-1)\alpha+\beta'}+O(1)\\
    &=\alpha\log\bigl(n(\alpha+O(n^{-1})\bigr)+\bigl((n-1)\alpha+\beta'\bigr)
    \log\frac{1+O(n^{-1})}{1+O(n^{-1})}+O(1)\\
    &=\alpha\log n+O(1)=\log n^\alpha+O(1).
\end{align*}
Thus,
\[
\log\frac{\Gamma(n\alpha+\beta')x^{-n}}%
{\Gamma\bigl((n-1)\alpha+\beta'\bigr)x^{-n+1}}=\log{n^\alpha}{x}+O(1),
\]
showing that the terms of the asymptotic expansion decrease in magnitude until 
$n^\alpha\approx x$.

For large~$n$, there is a risk that $\Gamma(n\alpha-\beta+1)$ overflows.
For example, in IEEE double precision arithmetic, $\Gamma(172)=\mathtt{Inf}$.
Standard numerical libraries therefore provide a $\log\Gamma(x)$ function 
which will not overflow unless $x$ itself is close to overflowing.  We 
therefore store
\[
a_n=(-1)^n\sin\pi(n\alpha-\beta)
\quad\text{and}\quad
b_n=\log\Gamma(n\alpha-\beta+1),
\]
and compute the terms of the asymptotic expansion as
\[
a_nb_nx^{-n}=a_n\exp(b_n-n\log x).
\]
(Notice that $a_1=\sin\pi(\beta-\alpha)>0$ if $0<\beta-\alpha<1$.)

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Asymptotic expansion: positive real argument}
Putting $z=x>0$ in~\eqref{eq: RN(z)} gives
\[
R_N(x)=\frac{-x^{-N-1}}{2\pi i}\int_{-\infty}^{0^+}
\frac{e^w w^{(N+1)\alpha-\beta}}{1-x^{-1}w^\alpha}\,dw,
\]
where the Hankel contour must be such that $x<|w^\alpha|$ for all~$w$.
Since we want to consider large~$x$, we move the contour to the left of the 
pole at~$w=x^{1/\alpha}$, picking up a residue
\[
\res_{w=x^{1/\alpha}}\frac{e^w w^{(N+1)\alpha-\beta}}{1-x^{-1}w^\alpha}
    =\lim_{w\to x^{1/\alpha}}e^w w^{(N+1)\alpha-\beta}\,
    \frac{w-x^{1/\alpha}}{1-x^{-1}w^\alpha}
    =\frac{-x^{N+1}}{\alpha}\,x^{(1-\beta)/\alpha}\exp(x^{1/\alpha}),
\]
leading to the expansion
\[
E_{\alpha,\beta}(x)=x^{(1-\beta)/\alpha}\exp(x^{1/\alpha})
    +\frac{1}{\pi}\sum_{n=1}^Na_nb_nx^{-n}+\tilde R_N(x).
\]







A deeper analysis of the exponential asymptotics of~$E_{\alpha,\beta}$ shows
that~\cite[Theorem~2.2]{WongZhao2002}
\[
E_{\alpha,\beta}(-x)=\frac{1}{\pi}\sum_{n=1}^{[x^{1/\alpha}]}
    A_nB_nx^{-n}+O\bigl(x^{1/2-\beta}e^{-x}\bigr)
\]
for $x>0$ and $0<\alpha\le 1-\epsilon$, where
\[
A_n=(-1)^{n+1}\sin\pi(n\alpha-\beta)
\quad\text{and}\quad
B_n=\Gamma(n\alpha-\beta+1).
\]
To reduce the risk of overflow for large~$n$, we store $\log B_n$ and compute
\[
B_nx^{-n}=\exp(\log B_n-n\log x).
\]
For optimal truncation, we choose $N$ so that if $x=N^\alpha$ (and hence
$N\approx x^{1/\alpha}$) then $B_Nx^{-N}=B_N/N^{N\alpha}<\epsilon$, or 
equivalently,
\[
N\alpha\log N-\log B_N>\log\epsilon^{-1}.
\]

Replacing $\beta$ with $\beta-r\alpha$ in the identity~\eqref{eq: beta identity}
gives
\[
z^rE_{\alpha,\beta}(z)=E_{\alpha,\beta-r\alpha}(z)-\sum_{n=0}^{r-1}
\frac{z^n}{\Gamma(\beta-r\alpha+n\alpha)},
\]
and so
\[
E_{\alpha,\beta}(z)=z^{-r}E_{\alpha,\beta-r\alpha}(z)-\sum_{n=0}^{r-1}
\frac{z^{-(r-n)}}{\Gamma\bigl(\beta-(r-n)\alpha\bigr)},
\]
leading to the asymptotic expansion
\[
E_{\alpha,\beta}(z)=-\sum_{n=1}^r\frac{z^{-n}}{\Gamma(\beta-n\alpha)}
    +O(z^{-r-1}).
\]
Recalling that the Gamma function satisfies
\[
\Gamma(z)\Gamma(1-z)=\frac{\pi}{\sin\pi z},
\]
it follows that
\[
\frac{-1}{\Gamma(\beta-n\alpha)}=\frac{1}{\pi}\,\sin\pi(n\alpha-\beta)
    \Gamma(1+n\alpha-\beta),
\]
so
\[
E_{\alpha,\beta}(z)=\frac{1}{\pi}\sum_{n=1}^r\sin\pi(n\alpha-\beta)
    \Gamma(1+n\alpha-\beta)z^{-n}+O(z^{-r-1}).
\]






%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Chebyshev expansion}
Consider the substitution
\[
y=\frac{x-1}{x+1}\quad\text{for $0\le x<\infty$.}
\]
The function~$x\mapsto y$ maps the half-line~$[0,\infty)$ onto~$[-1,1)$, and 
the inverse function~$y\mapsto x$ is given by
\[
x=\frac{1+y}{1-y}\quad\text{for $-1\le y<1$.}
\]
We define a function~$g$ by requiring that
\[
E_{\alpha,\beta}(-x)=\tfrac12(1-y)[1+(1+y)g(y)],
\]
or equivalently,
\[
g(y)=\frac{1}{1+y}\biggl(\frac{2E_{\alpha,\beta}(-x)}{1-y}-1\biggr).
\]
Notice that
\[
1+y=\frac{2x}{x+1}\quad\text{and}\quad 1-y=\frac{2}{x+1}.
\]

On the one hand, the power series~\eqref{eq: E alpha beta def} implies that
\[
g(y)=\frac{1}{2}\biggl(1-\frac{1}{\Gamma(\beta+\alpha)}\biggr)+O(x)
\quad\text{as $y\to-1$ (so $x\to0$),}
\]
and on the other hand, the asymptotic expansion~\eqref{eq: E minus asymptotic}
implies that
\[
g(y)=\frac{1}{2}\biggl(-1+\frac{1}{\pi}\,\sin\pi(\beta-\alpha)\,
    \Gamma(1+\alpha-\beta)\biggr)+O(x^{-1})\quad
\text{as $y\to1$ (so $x\to\infty$).}
\]
In particular, $g$ and all its derivatives are continuous on~$[-1,1]$.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Negative real argument when $1<\alpha<2$}
Assume for this section that $x>0$ and $1<\alpha<2$.  If follows that $F(w,x)$
has poles at $\gamma_\pm=x^{1/\alpha}e^{\pm i\pi/\alpha}$, 
with $\arg\gamma_\pm=\pm\pi/\alpha$ and hence
\[
\frac{\pi}{2}<\arg\gamma_+<\pi
\quad\text{and}\quad
-\pi<\arg\gamma_-<-\frac{\pi}{2}.
\]
Let
\[
G_\pm(w,x)=\frac{w-\gamma_\pm}{w^\alpha+x},
\]
so that
\[
F(w,x)=\frac{w^{\alpha-\beta}}{2}\biggl(\frac{G_+(w,x)}{w-\gamma_+}
    +\frac{G_-(w,x)}{w-\gamma_-}\biggr)
\]
with
\[
G_\pm(\gamma_\pm,x)=\lim_{x\to\gamma_\pm}G_\pm(w,x)
    =\frac{\gamma_\pm^{1-\alpha}}{\alpha}.
\]
Putting
\[
H_\pm(w,x)=\frac{G_\pm(w,x)-G_\pm(\gamma_\pm,x)}{w-\gamma_\pm}
    =\frac{1}{w^\alpha+x}-\frac{\gamma_\pm^{1-\alpha}}{\alpha(w-\gamma_\pm)},
\]
we have
\[
\frac{G_\pm(w,x)}{w-\gamma_+}=\frac{\gamma_\pm^{1-\alpha}/\alpha}{w-\gamma_\pm}
    +H_\pm(w,x)
\]
so
\[
F(w,x)=\frac{w^{\alpha-\beta}}{2}\biggl(
     \frac{\gamma_+^{1-\alpha}/\alpha}{w-\gamma_+}
    +\frac{\gamma_-^{1-\alpha}/\alpha}{w-\gamma_-}\biggr)
    +\frac{w^{\alpha-\beta}}{2}\bigl(H_+(w,x)+H_-(w,x)\bigr)
\]
and hence
\begin{align*}
E_{\alpha,\beta}(-x)&=\frac{1}{2\pi i}\int_{-\infty}^{0^+}e^wF(w,x)\,dw\\
&=\frac{1}{2\alpha}\,\bigl(\gamma_+^{1-\beta}e^{\gamma_+}
    +\gamma_-^{1-\beta}e^{\gamma_-}\bigr)
     +\frac{1}{2\pi i}\int_{-\infty}^{0^+}
    e^w\frac{w^{\alpha-\beta}}{2} \bigl(H_+(w,x)+H_-(w,x)\bigr)\,dw.
\end{align*}
Since
\begin{align*}
\gamma_+^{1-\beta}e^\gamma_+&=x^{(1-\beta)/\alpha}
    \exp\bigl(i\pi(1-\beta)/\alpha\bigr)
    \exp\bigl(x^{1/\alpha}(\cos\pi/\alpha+i\sin\pi/\alpha)\bigr)\\
    &=x^{(1-\beta)/\alpha}\exp(x^{1/\alpha}\cos\pi/\alpha)
    \exp\bigl(i\pi(1-\beta)/\alpha+ix^{1/\alpha}\sin\pi/\alpha\bigr)
\end{align*}
it follows that
\[
\frac{1}{2\alpha}\,\bigl(\gamma_+^{1-\beta}e^{\gamma_+}
    +\gamma_-^{1-\beta}e^{\gamma_-}\bigr)
    =\frac{x^{(1-\beta)/\alpha}}{\alpha}\,\exp(x^{1/\alpha}\cos\pi/\alpha)
    \cos\bigl(\pi(1-\beta)/\alpha+x^{1/\alpha}\sin\pi/\alpha\bigr).
\]
Also, recalling that $\gamma_\pm^\alpha=-x$,
\begin{align*}
H_\pm(\gamma_\pm,x)&=\lim_{w\to\gamma_\pm}
\frac{\alpha(w-\gamma_\pm)-\gamma_\pm^{1-\alpha}(w^\alpha+x)}%
{\alpha(w^\alpha+x)(w-\gamma_\pm)}\\
    &=\lim_{w\to\gamma_\pm}\frac{1-\gamma_\pm^{1-\alpha}w^{\alpha-1}}%
{\alpha w^{\alpha-1}(w-\gamma_\pm)+(w^\alpha+x)}\\
&=\lim_{w\to\gamma_\pm}\frac{-(\alpha-1)\gamma_\pm^{1-\alpha}w^{\alpha-2}}%
{\alpha(\alpha-1)w^{\alpha-2}(w-\gamma_\pm)+2\alpha w^{\alpha-1}}\\
&=\frac{(1-\alpha)\gamma_\pm^{-1}}{2\alpha\gamma_\pm^{\alpha-1}}
=\frac{1-\alpha}{2\alpha\gamma_\pm^\alpha}=\frac{-(1-\alpha)}{2\alpha x}.
\end{align*}

To evaluate $H_\pm(w,x)$ for $w$ near~$\gamma_\pm$, write
\[
w=\gamma_+(1+\epsilon_\pm)\quad\text{where}\quad
\epsilon_\pm=\frac{w-\gamma_\pm}{\gamma_\pm}.
\]
In this way,
\[
w^\alpha+x=\gamma_\pm^\alpha(1+\epsilon_\pm)^\alpha+x
    =-x[(1+\epsilon_\pm)^\alpha-1 ]
    =-x\sum_{n=1}^\infty c_n\epsilon_\pm^n\quad\text{for $|\epsilon_\pm|<1$,}
\]
where
\[
c_n=\frac{\alpha}{1}\cdot\frac{\alpha-1}{2}\cdots\frac{\alpha-n+1}{n},
\]
or in other words,
\[
c_1=\alpha\qquad\text{and}\qquad c_n=-\frac{n-\alpha-1}{n}\,c_{n-1}
\quad\text{for $n\ge2$.}
\]
Since
\begin{align*}
\alpha(w-\gamma_\pm)-\gamma_\pm^{1-\alpha}(w^\alpha+x)
    &=\alpha\gamma_\pm\epsilon_\pm+\gamma_\pm^{1-\alpha}x
        \biggl(c_1\epsilon_\pm+\sum_{n=2}^\infty c_n\epsilon_\pm^n\biggr)\\
    &=\alpha\gamma_\pm\epsilon_\pm-\gamma_\pm
        \biggl(\alpha\epsilon_\pm+\sum_{n=2}^\infty c_n\epsilon_\pm^n\biggr)\\
    &=-\gamma_\pm\sum_{n=2}^\infty c_n\epsilon_\pm^n
    =-\gamma_\pm\epsilon_\pm^2\sum_{n=0}^\infty c_{n+2}\epsilon_\pm^n
\end{align*}
and
\[
\alpha(w^\alpha+x)(w-\gamma_\pm)=-\alpha x\biggl(\sum_{n=1}^\infty 
    c_n\epsilon_\pm^n\biggr)\gamma_\pm\epsilon_\pm
    =-\alpha x\gamma_\pm\epsilon_\pm^2\sum_{n=0}^\infty c_{n+1}\epsilon_\pm^n,
\]
we see that
\[
H_\pm(w,x)=\biggl(\sum_{n=0}^\infty c_{n+2}\epsilon_\pm^n\biggr)\bigg/
    \biggl(\alpha x\sum_{n=0}^\infty c_{n+1}\epsilon_\pm^n\biggr)
    \quad\text{for $|x-\gamma_\pm|<|\gamma_\pm|=x^{1/\alpha}$.}
\]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Pad\'e approximation}
We wish to find a rational function of the form
\[
R(x)=\frac{p(x)}{q(x)}
=\frac{p_0+p_1x+p_2x+\cdots+p_rx^r}{q_0+q_1x+q_2x^2+\cdots+q_sx^r}
\]
such that
\[
E_{\alpha,\beta}(-x)=R(x)+\begin{cases}
    O(x^m)&\text{as $x\to0$,}\\
    O(x^{-n})&\text{as $x\to\infty$.}
\end{cases}
\]
Let
\[
a(x)=\sum_{j=0}^{m-1}a_kx^k\quad\text{and}\quad
b(x)=\sum_{j=1}^{n-1}b_kx^k
\]
and
\[
a_k=\frac{(-1)^k}{\Gamma(\beta+k\alpha)}
\quad\text{and}\quad
b_k=\frac{(-1)^{k+1}}{\Gamma(\beta-k\alpha)}
    =\frac{(-1)^k}{\pi}\,\sin\pi(k\alpha-\beta)\Gamma(1+k\alpha-\beta),
\]
so that
\[
E_{\alpha,\beta}(-x)=\begin{cases}
    a(x)+O(x^m)&\text{as $x\to0$,}\\
    b(x^{-1})+O(x^{-n})&\text{as $x\to\infty$.}
\end{cases}
\]
Thus, we require
\begin{equation}\label{eq: p q 0}
\frac{p(x)}{q(x)}-a(x)=O(x^m)\quad\text{as $x\to0$,}
\end{equation}
and
\begin{equation}\label{eq: p q oo}
\frac{x^{-r}p(x)}{x^{-r}q(x)}-b(x^{-1})=O(x^{-n})\quad\text{as $x\to\infty$,}
\end{equation}
or equivalently,
\[
p(x)-a(x)q(x)=O(x^m)\quad\text{as $x\to0$,}
\]
and
\[
x^{-r}p(x)-b(x^{-1})x^{-r}q(x)=O(x^{-n})\quad\text{as $x\to\infty$.}
\]
Equating coefficients of $x^k$ for~$0\le k\le m-1$ in~\eqref{eq: p q 0} and
of~$x^{-k}$ for~$0\le k\le n-1$ in~\eqref{eq: p q oo} gives $m+n$~equations
to determine the $2(r+1)$~coefficients of $p$~and $q$.  However, since the 
ratio~$p/q$ is unchanged if numerator and denominator are both multiplied by a 
common non-zero factor, we are free to put~$q_0=1$ leaving $2r+1$~unknowns.  
Thus, we arrive at the constraint
\[
m+n=2r+1.
\]

If $m\le r+1$, then
\begin{align*}
p(x)-a(x)q(x)&=\sum_{k=0}^rp_kx^k-\sum_{\ell=0}^{m-1}a_\ell x^\ell
    \sum_{j=0}^r q_jx^j\\
&=\sum_{k=0}^{m-1}\biggl(p_k-\sum_{j=0}^k a_{k-j}q_j\biggr)x^k
    +O(x^m)\quad\text{as $x\to0$,}
\end{align*}
whereas if $m>r+1$, then
\[
p(x)-a(x)q(x)=\sum_{k=0}^r\biggl(p_k-\sum_{j=0}^k a_{k-j}q_j\biggr)x^k
    -\sum_{k=r+1}^{m-1}\biggl(\sum_{j=0}^r a_{k-j}q_j\biggr)x^k+O(x^m).
\]
Setting $b_0=0$, if $n\le r+1$, then
\begin{align*}
x^{-r}p(x)-b(x^{-1})x^{-r}q(x)&=\sum_{k=0}^r p_kx^{k-r}
    -\sum_{\ell=0}^{n-1}b_\ell x^{-\ell}\sum_{j=0}^r q_jx^{j-r}\\
    &=\sum_{k=r-n+1}^r\biggl(p_k-\sum_{j=k}^rb_{j-k}q_j\biggr)x^{k-r}
    +O(x^{-n})\quad\text{as $x\to\infty$j}
\end{align*}
whereas if $n>r+1$, then
\[
x^{-r}p(x)-b(x^{-1})x^{-r}q(x)
    =\sum_{k=0}^r\biggl(p_k-\sum_{j=k}^rb_{j-k}q_j\biggr)x^{k-r}
    -\sum_{k=r-n+1}^{-1}\biggl(\sum_{j=0}^rb_{j-k}q_j\biggr)x^{k-r}+O(x^{-n}).
\]

If $m>r+1$ then $n=2r+1-m<r$ so the coefficients of $p(x)$~and $q(x)$ 
must satisfy the system of linear equations
\[
\begin{aligned}
p_k-\sum_{j=0}^ka_{k-j}q_j&=0&&\text{for $0\le k\le r$,}\\
\sum_{j=0}^r a_{k-j}q_j&=0&&\text{for $r+1\le k\le m-1$,}\\
p_k-\sum_{j=k}^rb_{j-k}q_j&=0&&\text{for $r-n+1\le k\le r$.}\\
\end{aligned}
\]
If $m=r+1$ then $n=r$ so
\[
\begin{aligned}
p_k-\sum_{j=0}^ka_{k-j}q_j&=0&&\text{for $0\le k\le r$,}\\
p_k-\sum_{j=k}^rb_{j-k}q_j&=0&&\text{for $1\le k\le r$.}
\end{aligned}
\]
If $m=r$ then $n=r+1$ so
\[
\begin{aligned}
p_k-\sum_{j=0}^ka_{k-j}q_j&=0&&\text{for $0\le k\le r-1$,}\\
p_k-\sum_{j=k}^rb_{j-k}q_j&=0&&\text{for $0\le k\le r$.}
\end{aligned}
\]
Finally, if $m<r$ then $n>r+1$ so
\[
\begin{aligned}
p_k-\sum_{j=0}^ka_{k-j}q_j&=0&&\text{for $0\le k\le m-1$,}\\
\sum_{j=0}^rb_{j-k}q_j&=0&&\text{for $r-n+1\le k\le-1$,}\\
p_k-\sum_{j=k}^rb_{j-k}q_j&=0&&\text{for $0\le k\le r$.}
\end{aligned}
\]
Notice that in all four cases, the first equation and the last two equations 
read
\begin{align*}
p_0-a_0q_0&=0,\\
p_{r-1}-b_0q_{r-1}-b_1q_r&=0,\\
p_r-b_0q_r&=0.
\end{align*}
Since we fixed $q_0=1$~and $b_0=0$, it follows that
\[
p_0=a_0=\frac{1}{\Gamma(\beta)}\quad\text{and}\quad p_r=0.
\]
When $\beta=\alpha$, we have also $p_{r-1}=0$ since $b_1=1/\Gamma(0)=0$.

1G\begin{example}
Suppose $r=3$ and we choose $m=r+1=4$ and $n=r=3$.  The linear system is
\[
\begin{aligned}
p_k-\sum_{j=1}^ka_{k-j}q_j&=a_k&&\text{for $1\le k\le 3$,}\\
p_k-\sum_{j=k+1}^3b_{j-k}q_j&=0&&\text{for $1\le k\le 3$,}
\end{aligned}
\]
or, in matrix form,
\[
\begin{bmatrix}
1& &-a_0&    &    \\
 &1&-a_1&-a_0&    \\
 & &-a_2&-a_1&-a_0\\
1& &    &-b_1&-b_2\\
 &1&    &    &-b_1
\end{bmatrix}
\begin{bmatrix}p_1\\ p_2\\ q_1\\ q_2\\ q_3\end{bmatrix}
=\begin{bmatrix}a_1\\ a_2\\ a_3\\ 0\\ 0\end{bmatrix},
\]
where we used the fact that $p_0=a_0$, $p_3=0$ and $q_0=1$.
\end{example}

\begin{figure}
\caption{Error $R(x)-E_{1/2}(x)$ for two choices of $m$~and $n$ when~$r=3$.}
\label{fig: pade 43 52 error}
\begin{center}
\includegraphics[scale=0.6]{../experiments/pade_43_52-crop.pdf}
\end{center}
\end{figure}

\begin{table}
\caption{Coefficients of $p(x)$~and $q(x)$ for two choices of $m$~and $n$
when $r=3$.}\label{tab: pade 43 52 coef}
\begin{center}
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{c|cc|cc}
   &\multicolumn{2}{c|}{$m=4$, $n=3$}
   &\multicolumn{2}{c}{$m=5$, $n=2$}\\
\hline
$k$&$p_k$&$q_k$&$p_k$&$q_k$\\
\hline
   0&  1.00000&  1.00000&  1.00000&  1.00000\\
   1&  0.76401&  1.89239&  0.61721&  1.74559\\
   2&  0.21884&  1.35417&  0.15656&  1.12625\\
   3&  0.00000&  0.38788&  0.00000&  0.27750
\end{tabular}
\end{center}
\end{table}

\begin{example}
For comparison, suppose again that $r=3$ but take $m=5$ and $n=2$.  This time,
\[
\begin{aligned}
p_k-\sum_{j=1}^ka_{k-j}q_j&=a_k&&\text{for $1\le k\le 3$,}\\
-\sum_{j=1}^3a_{k-j}q_j&=a_k&&\text{for $k=4$,}\\
p_k-\sum_{j=k+1}^3b_{j-k}q_j&=0&&\text{for $2\le k\le 3$,}
\end{aligned}
\]
or
\[
\begin{bmatrix}
1& &-a_0&    &    \\
 &1&-a_1&-a_0&    \\
 & &-a_2&-a_1&-a_0\\
 & &-a_3&-a_2&-a_1\\
 &1&    &    &-b_1
\end{bmatrix}
\begin{bmatrix}p_1\\ p_2\\ q_1\\ q_2\\ q_3\end{bmatrix}
=\begin{bmatrix}a_1\\ a_2\\ a_3\\ a_4\\ 0\end{bmatrix},
\]
with $p_0=a_0$, $p_3=0$ and $q_0=1$.  With $\alpha=1/2$ and $\beta=1$, the 
errors are plotted in \cref{fig: pade 43 52 error}, and the values of the 
coefficients are given in \cref{tab: pade 43 52 coef}.
\end{example}

\begin{example}
If $r=3$, $m=2$ and $n=5$, then
\[
\begin{aligned}
p_k-\sum_{j=1}^ka_{k-j}q_j&=a_k&&\text{for $1\le k\le 1$,}\\
-\sum_{j=1}^3 b_{j+k}q_j&=b_k&&\text{for $1\le k\le1$,}\\
-\sum_{j=1}^3 b_jq_j&=-a_0,&&\\
p_k-\sum_{j=k+1}^3b_{j-k}q_j&=0&&\text{for $1\le k\le 2$,}
\end{aligned}
\]
or
\[
\begin{bmatrix}
1& &-a_0&    &\\
 & &-b_2&-b_3&-b_4\\
 & &-b_1&-b_2&-b_3\\
1& &    &-b_1&-b_2\\
 &1&    &    &-b_1
\end{bmatrix}
\begin{bmatrix}p_1\\ p_2\\ q_1\\ q_2\\ q_3\end{bmatrix}
=\begin{bmatrix}a_1\\ b_1\\ -a_0\\ 0\\ 0\end{bmatrix},
\]

\end{example}

In general, after eliminating $p_0=a_0$, $p_r=0$ and $q_0=1$, we have to solve 
a square linear system of order~$2r-1=m+n-2$ for the unknowns $p_1$, \ldots,
$p_{r-1}$ and $q_1$, \ldots, $q_r$.  If $m\ge r+1$ and thus $n\le r$, then the
system takes the form
\[
\begin{aligned}
p_k-\sum_{j=1}^k a_{k-j}q_j&=a_k&&\text{for $1\le k\le r-1$,}\\
-\sum_{j=1}^r a_{k-j}q_j&=a_k&&\text{for $r\le k\le m-1$,}\\
p_k-\sum_{j=k+1}^r b_{j-k}q_j&=0&&\text{for $r-n+1\le k\le r-1$,}
\end{aligned}
\]
where we used the fact that $b_0=0$. If, instead, $m\le r$ and thus $n\ge r+1$, 
then
\[
\begin{aligned}
p_k-\sum_{j=1}^k a_{k-j}q_j&=a_k&&\text{for $1\le k\le m-1$,}\\
-\sum_{j=1}^r b_{j+k}q_j&=b_k&&\text{for $1\le k\le n-r-1$,}\\
-\sum_{j=1}^r b_jq_j&=-a_0,&&\\
p_k-\sum_{j=k+1}^r b_{j-k}q_j&=0&&\text{for $1\le k\le r-1$.}
\end{aligned}
\]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Using the SVD}
Instead of requiring $q_0=1$, we can instead adapt the approach of Gonnet et 
al.~\cite{GonnetEtAl2013} and use the SVD to solve the resulting underdetermined
$2r\times(2r+1)$ linear system.  (We must still have $p_r=0$.) If $m\ge r+1$ 
then $n=2r+1-m\le r$ so
\[
\begin{aligned}
p_k-\sum_{j=0}^k a_{k-j}q_j&=0&&\text{for $0\le k\le r-1$,}\\
-\sum_{j=0}^r a_{k-j}q_j&=0&&\text{for $r\le k\le m-1$,}\\
p_k-\sum_{j=k+1}^rb_{j-k}q_j&=0&&\text{for $r-n+1\le k\le r-1$,}
\end{aligned}
\]
or in matrix form,
\[
\begin{bmatrix}A_{11}&A_{12}\\ A_{21}&A_{22}\\ A_{31}&A_{32}\end{bmatrix}
\begin{bmatrix}\boldsymbol{p}\\ \boldsymbol{q}\end{bmatrix}=\boldsymbol{0},
\qquad\boldsymbol{p}=\begin{bmatrix}p_0\\ p_1\\ \vdots\\ p_{r-1}\end{bmatrix},
\qquad\boldsymbol{q}=\begin{bmatrix}q_0\\ q_1\\ \vdots\\ q_{r-1}\\q_r
\end{bmatrix},
\]
where
\[
A_{11}=I_{r\times r},\qquad
A_{21}=O_{(m-r)\times r},\qquad
A_{31}=[O_{(n-1)\times(r-n)}\quad I_{(n-1)\times(n-1)}],
\]
with
\begin{gather*}
A_{12}=\kbordermatrix{
      &1       &2       &\cdots&r     &r+1\\     
1     &-a_0    &0       &\cdots&0     &0\\
2     &-a_1    &-a_0    &\cdots&0     &0\\
\vdots&\vdots  &\vdots  &\ddots&\vdots&\vdots\\
r     &-a_{r-1}&-a_{r-2}&\cdots&-a_0&0
},\\
A_{22}=\kbordermatrix{
      &1       &2       &\cdots&r         &r+1\\
1     &-a_r    &-a_{r-1}&\cdots&-a_1      &-a_0\\
2     &-a_{r+1}&-a_r    &\cdots&-a_2      &-a_1\\
\vdots&\vdots  &\vdots  &\ddots&\vdots    &\vdots\\
m-r   &-a_{m-1}&-a_{m-2}&\cdots&-a_{m-r-2}&-a_{m-r-1}
}\\
A_{32}=\kbordermatrix{
      &1     &\cdots&r-n+2 &r-n+3 &\cdots&r+1\\
1     &0     &\cdots&-b_1  &-b_2  &\cdots&-b_{n-1}\\
2     &0     &\cdots&0     &-b_1  &\cdots&-b_{n-2}\\
\vdots&\vdots&\ddots&\vdots&\vdots&\ddots&\vdots\\
n-1   &0     &\cdots&0     &0     &\cdots&-b_1
}.
\end{gather*}
We compute the full SVD, $A=USV^\top$, and since the final column of the 
$(2r)\times(2r+1)$ diagonal matrix~$S$ is a zero vector it follows that
\[
S\boldsymbol{e}_{2r+1}=\boldsymbol{0}.
\]
Thus, the unit vector 
\[
\boldsymbol{y}=V\boldsymbol{e}_{2r+1}=\text{column $2r+1$ of $V$}
\]
satisfies $A\boldsymbol{y}=USV^\top V\boldsymbol{e}_{2r+1}
=US\boldsymbol{e}_{2r+1}=\boldsymbol{0}$.  For any desired scale 
factor~$\mu\ne0$, we may put
\[
p_k=\mu y_{k+1}\quad\text{for $0\le k\le r-1$}
\]
and
\[
q_k=\mu y_{r+1+k}\quad\text{for $0\le k\le r$.}
\]
In particular, choosing $\mu=1/y_{r+1}$ ensures $q_0=1$.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\printbibliography
\end{document}

