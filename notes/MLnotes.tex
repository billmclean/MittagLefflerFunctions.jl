\documentclass[12pt,a4paper]{article}
\usepackage[margin=2cm]{geometry}
\usepackage{amsmath,amsfonts,amsthm}
\usepackage{graphicx}
\usepackage{biblatex}
\usepackage{hyperref}
\usepackage{cleveref}
\bibliography{MLnotes}
\title{Notes on the Mittag--Leffler function}
\author{William McLean}
\date{\today}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{example}[theorem]{Example}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\DeclareMathOperator*{\res}{res}
\newcommand{\arcosh}{\operatorname{arcosh}}
\newcommand{\diag}{\operatorname{diag}}
\newcommand{\argmin}{\operatorname*{arg\,min}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% % %
\begin{document}
\maketitle
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
The two-argument, Mittag--Leffler function is defined by the power series
\begin{equation}\label{eq: E alpha beta def}
E_{\alpha,\beta}(z)=\sum_{n=0}^\infty\frac{z^n}{\Gamma(\beta+n\alpha)}
\quad\text{for $z\in\mathbb{C}$, $\alpha\ge0$, $\beta\in\mathbb{R}$.}
\end{equation}
The reciprocal of the Gamma function may be written as an integral along a
Hankel contour,
\[
\frac{1}{\Gamma(\beta+n\alpha)}=\frac{1}{2\pi i}\int_{-\infty}^{0^+}
    \frac{e^w\,dw}{w^{\beta+n\alpha}},
\]
where we take a branch cut along the negative real axis so that 
$-\pi<\arg w<\pi$. Therefore, if $|z|<|w^\alpha|$ for all~$w$ on the contour, 
then
\[
E_{\alpha,\beta}(z)=\frac{1}{2\pi i}\int_{-\infty}^{0^+}\frac{e^w}{w^\beta}
    \sum_{n=0}^\infty(zw^{-\alpha})^n\,dw
    =\frac{1}{2\pi i}\int_{-\infty}^{0^+}\frac{e^w}{w^\beta}\,
    \frac{dw}{1-zw^{-\alpha}},
\]
and so
\begin{equation}\label{eq: integral repn}
E_{\alpha,\beta}(z)=\frac{1}{2\pi i}\int_{-\infty}^{0^+}
    \frac{e^w\,dw}{w^\beta-zw^{\beta-\alpha}}.
\end{equation}

If $\alpha>0$, then it suffices to treat the case~$\beta>0$ because the identity
\begin{equation}\label{eq: beta identity}
z^rE_{\alpha,\beta+r\alpha}=E_{\alpha,\beta}(z)
    -\sum_{n=0}^{r-1}\frac{z^n}{\Gamma(\beta+n\alpha)},\quad 
r\in\{1,2,3,\ldots\},
\end{equation}
allows us to express $E_{\alpha,\beta}(z)$ in terms 
of~$E_{\alpha,\beta+r\alpha}(z)$.  Also, in the case~$\alpha=0$,
\[
E_{0,\beta}(z)=\frac{1}{\Gamma(\beta)(1+z)}.
\]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Taylor approximation for $z$ near zero}
Since 
$\Gamma(t)\ge\Gamma(1{\cdot}46163\,21456\,31155)=0{\cdot}88560\,31944\,10889
=1/1{\cdot}12917\,38854\,50141$, a crude bound for the remainder term~$R_N(z)$ 
in the Taylor expansion,
\[
E_{\alpha,\beta}(z)=\sum_{n=0}^N\frac{z^n}{\Gamma(\beta+n\alpha)}+R_N(z)
\]
is given by
\[
|R_N(z)|\le1{\cdot}3\sum_{n=N+1}^\infty|z|^n=\frac{1{\cdot}3\,|z|^{N+1}}{1-|z|}
\quad\text{for $|z|<1$.}
\]
Hence, if
\[
N+1\ge\frac{\log\bigl((1-|z|)\epsilon/1{\cdot}3\bigr)}{\log|z|}
\quad\text{and}\quad|z|<1,
\]
then $|R_N(z)|\le\epsilon$.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Quadrature approximation: negative real argument}\label{sec: quad neg}
Assume now that $0<\alpha<1$. Let $x>0$ and put $z=-x$ 
in~\eqref{eq: integral repn} to obtain
\[
E_{\alpha,\beta}(-x)=\frac{1}{2\pi i}\int_{-\infty}^{0^+}
    e^wF(w,x)\,dw
\quad\text{where}\quad
F(w,x)=\frac{w^{\alpha-\beta}}{w^\alpha+x}.
\]
We note that since $|\arg(w)|<\pi$ if follows that $|\arg(w^\alpha)|<\alpha\pi$
so $w^\alpha+x\ne0$.  Consider the Hankel contour parameterised 
by~\cite{WeidemanTrefethen2007}
\[
w(u)=\mu\bigl(1+\sin(iu-\phi)\bigr)\quad\text{for $-\infty<u<\infty$,}
\]
with $\mu>0$ and $0<\phi<\pi/2$. Since
\begin{align*}
\Re w&=\mu(1-\cosh u\,\sin\phi),\\
\Im w&=\mu\sinh u\,\cos\phi,
\end{align*}
we have
\[
\biggl(\frac{\Re w-1}{\mu\sin\phi}\biggr)^2
    -\biggl(\frac{\Im w}{\mu\cos\phi}\biggr)^2=1,
\]
showing that the contour is the left branch of an hyperbola with asymptotes
\[
\Im w=\pm(\Re w-1)\tan\phi.
\]
Thus,
\[
E_{\alpha,\beta}(-x)=\frac{1}{2\pi i}\int_{-\infty}^\infty 
e^{w(u)}F\bigl(w(u),x\bigr)z'(u)\,du\approx Q_h(x),
\]
where, for a step size~$h>0$, we put
\[
u_n=nh,\qquad w_n=w(u_n),\qquad w'_n=w'(u_n),
\]
and define the series approximation
\[
Q_h(x)=\frac{h}{2\pi i}\sum_{n=-\infty}^\infty e^{w_n}F(w_n,x)w'_n.
\]
For a fixed~$v$ with $0<\phi+v<\pi/2$, we see that
$w(u+iv)=\mu\bigl[1+\sin(iu-(\phi+v)\bigr)\bigr]$ parameterises the left 
branch of an hyperbola with asymptotes
\[
\Im w=\pm(\Re w-1)\tan(\phi+v).
\]
Putting
\[
M(x,v)=\int_{-\infty}^\infty\bigl|\exp\bigl(w(u+iv)\bigr)
    F\bigl(w(u+iv),x\bigr)w'(u+iv)\bigr|\,du
    \quad\text{for $-\phi<v<\frac{\pi}{2}$,}
\]
we have the error bound~\cite[Theorem~2.1]{WeidemanTrefethen2007}
\begin{equation}\label{eq: Qh error}
|Q_h(x)-E_{\alpha,\beta}(-x)|\le\frac{M(x,r)}{\exp(2\pi r/h)-1}
    +\frac{M(x,-s)}{\exp(2\pi s/h)-1}
\end{equation}
for $0<s<\phi$ and $0<r<\pi/2-\phi$.  For the finite sum,
\begin{equation}\label{eq: Q_hN}
Q_{h,N}(x)=\frac{h}{2\pi i}\sum_{n=-N}^N e^{w_n}F(w_n,x)w'_n,
\end{equation}
we have an additional truncation error
\[
T_{h,N}=h\sum_{n=N+1}^\infty\bigl(e^{w_n}F(w_n,x)w'_n
    +e^{w_{-n}}F(w_{-n},x)w'_{-n}\bigr).
\]
Choosing the largest possible values $s=\phi$~and $r=\pi/2-\phi$, the three
error terms are of order $\exp\bigl(-(\pi^2-2\pi\phi)/h\bigr)$,
$\exp(\mu-2\pi\phi/h)$ and $\exp\bigl(\mu(1-\cosh(Nh)\sin\phi)\bigr)$.
To balance these quantities, we seek to choose the parameters so that
\cite[(4.2)]{WeidemanTrefethen2007}
\[
-\frac{\pi^2-2\pi\phi}{h}=\mu-\frac{2\pi\phi}{h}
    =\mu\bigl(1-\cosh(Nh)\sin\phi\bigr).
\]
The left-hand equation implies that
\[
\mu=\frac{4\pi\phi-\pi^2}{h},
\]
which allows us to eliminate $\mu$ in the right-hand equation, to obtain
\[
\cosh(Nh)=\frac{2\pi\phi}{(4\phi-\pi)\pi\sin\phi}.
\]
We therefore define
\[
a(\phi)=\arcosh\biggl(\frac{2\pi\phi}{(4\phi-\pi)\pi\sin\phi}\biggr)
\quad\text{and}\quad b(\phi)=\frac{4\pi\phi-\pi^2}{A(\phi)}
\]
for $\pi/4<\phi<\pi/2$, and set
\[
h=\frac{a(\phi)}{N}\quad\text{and}\quad \mu=b(\phi)\,N,
\]
so that the error~$Q_{h,N}(x)-E_{\alpha,\beta}(-x)$ is order~$e^{-C(\phi)N}$ 
for
\begin{equation}\label{eq: C function}
C(\phi)=\frac{\pi^2-2\pi\phi}{A(\phi)}.
\end{equation}
\Cref{fig: C plot} shows a plot of~$C(\phi)$, and we find using a standard 
optimization package that the maximum value occurs at
\[
\phi=\phi_\star=1.172104\quad\text{with}\quad C(\phi_\star)=2.315654.
\]
Furthermore,
\[
a(\phi_\star)=1.081792\quad\text{and}\quad
b(\phi_\star)=4.492075,
\]
leading to the optimal values 
\[
h=h_\star=\frac{a(\phi_\star)}{N}\quad\text{and}\quad
\mu=\mu_\star=b(\phi_\star)\,N.
\]
Since $e^{B(\phi_\star)}=10.131547$ we expect to gain about an extra decimal
digit of accuracy each time we increase $N$ by~$1$.

\begin{figure}
\caption{Plot of $C(\phi)$ from~\eqref{eq: C function} showing the 
maximum at~$\phi_\star$.}\label{fig: C plot}
\begin{center}
\includegraphics[scale=0.75]{Bplot.pdf}
\end{center}
\end{figure}

Notice that
\[
\frac{hw'(u)}{2\pi i}=\frac{h\mu}{2\pi}\,\cos(iu-\phi)
    =\frac{a(\phi_\star)b(\phi_\star)}{2\pi}\,\cos(iu-\phi)
\]
so we can re-write the sum~\eqref{eq: Q_hN} as
\[
Q_{hN}(x)=\frac{a(\phi_\star)b(\phi_\star)}{2\pi}\sum_{n=-N}^N
    e^{w_n}F(w_n,x)\cos(iu_n-\phi).
\]
Also, $u_{-n}=-u_n$, $w(-u)=\overline{w(u)}$~and 
$F(\bar w,x)=\overline{F(w,x)}$, so the terms of the sum occur in complex 
conjugate pairs,
\[
e^{w_{-n}}F(w_{-n},x)\cos(iu_{-n}-\phi)
    =\overline{e^{w_n}F(w_n,x)\cos(iu_n-\phi)}.
\]
Thus,
\[
Q_{h,N}(x)=\frac{a(\phi_\star)b(\phi_\star)}{\pi}\biggl(
    \tfrac12e^{w_0}F(w_0,x)\cos\phi
    +\sum_{n=1}^N\Re\bigl(e^{w_n}F(w_n,x)\cos(iu_n-\phi)\bigr)\biggr),
\]
but note that $w_n$~and $u_n$ depend on~$N$ through $h$~and $\mu$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Quadrature approximation: positive real argument}
We now consider
\[
E_{\alpha,\beta}(x)=\frac{1}{2\pi i}\int_{-\infty}^{0^+}
    e^wF(w,-x)\,dx,
\]
where the Hankel contour must pass to the right of the pole at~$w=x^{1/\alpha}$.
Write
\[
F(w,-x)=w^{\alpha-\beta}\,\frac{\rho(w,x)}{w-x^{1/\alpha}}\quad\text{where}\quad
\rho(w,x)=\frac{w-x^{1/\alpha}}{w^\alpha-x},
\]
and observe that $\rho(w,x)$ has a removable singularity at~$w=x^{1/\alpha}$. 
In fact, using L'H\^ospital's rule,
\[
\rho(x^{1/\alpha},x)=\lim_{w\to x^{1/\alpha}}
    \frac{w-x}{w^\alpha-x}=\frac{x^{(1/\alpha)-1}}{\alpha},
\]
and if we define
\[
H(w;x)=\frac{w^{\alpha-\beta}\rho(w;x)
-(x^{1/\alpha})^{\alpha-\beta}\rho(x^{1/\alpha};x)}{w-x^{1/\alpha}}
    =\frac{w^{\alpha-\beta}}{w^\alpha-x}
    -\frac{\alpha^{-1}x^{(1-\beta)/\alpha}}{w-x^{1/\alpha}}
\]
then
\[
F(w;-x)=\frac{\alpha^{-1}x^{(1-\beta)/\alpha}}{w-x^{1/\alpha}}+H(w;x). 
\]
Hence,
\begin{align*}
E_{\alpha,\beta}(x)&=\frac{1}{2\pi i}\int_{-\infty}^{0^+}e^w F(w;-x)\,dw\\
    &=\frac{\alpha^{-1}x^{(1-\beta)/\alpha}}{2\pi i}\int_{-\infty}^{0^+}
    \frac{e^w\,dw}{w-x^{1/\alpha}}
    +\frac{1}{2\pi i}\int_{-\infty}^{0^+}e^w H(w;x)\,dw,
\end{align*}
that is,
\begin{equation}\label{eq: pos H}
E_{\alpha,\beta}(x)=\alpha^{-1}x^{(1-\beta)/\alpha}\exp(x^{1/\alpha})
    +\frac{1}{2\pi i}\int_{-\infty}^{0^+}e^w H(w,x)\,dw.
\end{equation}
The integral term in \eqref{eq: pos H} can be approximated using the same 
approach as in \Cref{sec: quad neg}, with $H(w,x)$ replacing $F(w,x)$.

To evaluate $H(w,x)$ for $w$ near~$x^{1/\alpha}$, put
\[
w=x^{1/\alpha}(1+\epsilon)\quad\text{where}\quad 
\epsilon=\frac{w-x^{1/\alpha}}{x^{1/\alpha}}
\]
and define
\[
\psi_{1,\alpha}(\epsilon)=\frac{(1+\epsilon)^\alpha-1}{\epsilon}
    =\sum_{k=1}^\infty\binom{\alpha}{k}\epsilon^{k-1}
\]
and
\[
\psi_{2,\alpha}(\epsilon)
    =\frac{(1+\epsilon)^\alpha-(1+\alpha\epsilon)}{\epsilon}
    =\sum_{k=2}^\infty\binom{\alpha}{k}\epsilon^{k-2}.
\]
In this way,
\[
w^\alpha-x=x\epsilon\psi_{1,\alpha}(\epsilon)
    =x\epsilon[\alpha+\epsilon\psi_{2,\alpha}(\epsilon)],
\]
so
\begin{align*}
H(w;x)&=\frac{w^{\alpha-\beta}(w-x^{1/\alpha})
-\alpha^{-1}x^{(1-\beta)/\alpha}(w^\alpha-x)}{(w^\alpha-x)(w-x^{1/\alpha})}\\
&=\frac{x^{(\alpha-\beta)/\alpha}(1+\epsilon)^{\alpha-\beta}
(x^{1/\alpha}\epsilon)-x^{(1-\beta)/\alpha}x\epsilon 
[1+\alpha^{-1}\epsilon\psi_{2,\alpha}(\epsilon)]}%
{x\epsilon\psi_{1,\alpha}(\epsilon)(x^{1/\alpha}\epsilon)}\\
&=\frac{x^{1+(1-\beta)/\alpha}\epsilon}{x^{1+(1/\alpha)}\epsilon}\,
\frac{[1+\epsilon\psi_{1,\alpha-\beta}(\epsilon)]
-[1+\alpha^{-1}\epsilon\psi_{2,\alpha}(\epsilon)]}%
{\epsilon\psi_{1,\alpha}(\epsilon)}\\
&=\frac{\psi_{1,\alpha-\beta}(\epsilon)-\alpha^{-1}\psi_{2,\alpha}(\epsilon)}%
{x^{\beta/\alpha}\psi_{1,\alpha}(\epsilon)},
\end{align*}
and thus
\[
H(x^{1/\alpha};x)=\frac{\psi_{1,\alpha-\beta}(0)
-\alpha^{-1}\psi_{2,\alpha}(0)}{x^{\beta/\alpha}\psi_{1,\alpha}(0)}
    =\frac{1+\alpha-2\beta}{2\alpha x^{\beta/\alpha}}.
\]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Asymptotic expansion: negative real argument}
Since
\[
\frac{1}{w^\beta-zw^{\beta-\alpha}}=\frac{-1}{zw^{\beta-\alpha}}
    \,\frac{1}{1-w^\alpha z^{-1}}
\]
and
\[
\frac{1}{1-w^\alpha z^{-1}}=\sum_{n=0}^{N-1}(w^\alpha z^{-1})^n
    +\frac{(w^\alpha z^{-1})^N}{1-w^\alpha z^{-1}},
\]
we see from~\eqref{eq: integral repn} that
\[
E_{\alpha,\beta}(z)=\sum_{n=0}^{N-1}\frac{-1}{2\pi i}\int_{-\infty}^{0^+}
    \frac{e^w(w^\alpha z^{-1})^n}{zw^{\beta-\alpha}}\,dw+R_N(z),
\]
where the remainder term is
\[
R_N(z)=\frac{-1}{2\pi i}\int_{-\infty}^{0^+}
    \frac{e^w(w^\alpha z^{-1})^N\,dw}{zw^{\beta-\alpha}(1-w^\alpha z^{-1})}.
\]
The $n$th term equals
\[
\frac{-z^{-1-n}}{2\pi i}\int_{-\infty}^{0^+}e^w w^{(n+1)\alpha-\beta}\,dw,
\]
and if we assume $\alpha-\beta>-1$ then by collapsing the contour onto the 
negative real axis and using the substitutions~$w=re^{\pm i\pi}$, we obtain
\[
\frac{-1}{2\pi i}\int_{-\infty}^{0^+}e^w w^{(n+1)\alpha-\beta}\,dw
    =\frac{e^{i\pi[(n+1)\alpha-\beta]}-e^{-i\pi[(n+1)\alpha-\beta]}}{2\pi i}
    \int_0^\infty e^{-r}r^{(n+1)\alpha-\beta}\,dr
\]
so
\[
E_{\alpha,\beta}(z)=R_N(z)+\frac{1}{\pi}\sum_{n=0}^{N-1}
    \sin\pi[(n+1)\alpha-\beta]\,\Gamma\bigl((n+1)\alpha-\beta+1\bigr)z^{-n-1}.
\]
Also,
\begin{equation}\label{eq: RN(z)}
R_N(z)=\frac{-z^{-N-1}}{2\pi i}\int_{-\infty}^{0^+}
    \frac{e^w w^{(N+1)\alpha-\beta}}{1-z^{-1}w^\alpha}\,dw.
\end{equation}

Suppose $z=-x<0$, and choose $\theta\in(0,\pi/2)$.  Define the semi-infinite
lines
\[
\varGamma_\pm=\{\,re^{\pm i(\pi-\theta)}:0<r<\infty\,\},
\]
and consider
\[
R_N(-x)=\frac{(-1)^Nx^{-N-1}}{2\pi i}\int_{-\infty}^{0+}
    \frac{e^w w^{(N+1)\alpha-\beta}}{1+x^{-1}w^\alpha}\,dw.
\]
By integrating along $-\varGamma_-$ and $\varGamma_+$, we see that
\[
|R_N(-x)|\le\frac{x^{-N-1}}{\pi}\int_0^\infty
\frac{e^{-r\cos\theta}r^{(N+1)\alpha-\beta}}%
{|1+x^{-1}r^\alpha e^{i\phi}|}\,dr\quad\text{where}\quad
\phi=(\pi-\theta)\alpha.
\]
Since
\[
|1+x^{-1}r^\alpha e^{i\phi}|\ge\Re(1+x^{-1}r^\alpha\cos\phi)
    =1+x^{-1}r^\alpha\cos\phi,
\]
if $0<\phi\le\pi/2$ then $\cos\phi\ge0$ and hence $|1+x^{-1}r^\alpha|\ge1$ so,
using the substitution~$y=r\cos\theta$,
\begin{align*}
|R_N(-x)|&\le\frac{x^{-N-1}}{\pi}\int_0^\infty 
    e^{-r\cos\theta}r^{(N+1)\alpha-\beta}\,dr
    =\frac{x^{-N-1}}{\pi}\,\frac{1}{(\cos\theta)^{(N+1)\alpha-\beta+1}}
    \int_0^\infty e^{-y}y^{(N+1)\alpha-\beta}\,dy\\
    &=\frac{(\cos\theta)^{1-\beta}}{\pi}\,
    \bigl(x(\cos\theta)^\alpha\bigr)^{-N-1}\,
    \Gamma\bigl((N+1)\alpha-\beta+1\bigr)=O(x^{-N-1}).
\end{align*}
Otherwise, if $\pi/2<\phi<\pi$, then $\cos\phi<0$ and we put $q=x/|\cos\phi|$ 
so that
\[
|1+x^{-1}r^\alpha e^{i\phi}|\ge1/2\quad\text{for}\quad 0<r<(q/2)^{1/\alpha}
\quad\text{and}\quad (3q/2)^{1/\alpha}<r<\infty,
\]
whereas, for $(q/2)^{1/\alpha}\le r\le(3q/2)^{1/\alpha}$, we use the lower bound
\[
|1+x^{-1}r^\alpha e^{i\phi}|\ge|\Im(1+x^{-1}r^\alpha e^{i\phi})|
    =x^{-1}r^\alpha|\sin\phi|.
\]
Thus, $|R_N(-x)|\le I_1+I_2$ where $I_1=O(x^{-N-1})$ and
\begin{align*}
I_2&=\frac{x^{-N}}{\pi\sin\phi}\int_{(q/2)^{1/\alpha}}^{(3q/2)^{1/\alpha}}
    e^{-r\cos\theta}r^{N\alpha-\beta}\,dr\\
&\le\frac{x^{-N}}{\pi\sin\phi}\,\exp\bigl(-(q/2)^{1/\alpha}\cos\theta\bigr)
    \biggl[\frac{r^{N\alpha-\beta+1}}{N\alpha-\beta+1}
    \biggr]_{(q/2)^{1/\alpha}}^{(3q/2)^{1/\alpha}}\\
&\le\frac{x^{-N-1}q\cos\phi}{\pi\sin\phi}\,(3q/2)^{N+(1-\beta)/\alpha}\,
\frac{\exp\bigl(-(q/2)^{1/\alpha}\cos\theta\bigr)}{N\alpha-\beta+1}
=O(x^{-N-1}),
\end{align*}
yielding the asymptotic expansion
\[
E_{\alpha,\beta}(-x)=\frac{1}{\pi}\sum_{n=1}^N(-1)^n
    \sin\pi(n\alpha-\beta)\,\Gamma(n\alpha-\beta+1)\,x^{-n}
    +O(x^{-N-1})\quad\text{as $x\to\infty$.}
\]
Notice that the identity
\[
\Gamma(z)\,\Gamma(1-z)=\frac{\pi}{\sin\pi z}
\]
allows us to re-write the expansion as
\[
E_{\alpha,\beta}(-x)=-\sum_{n=1}^N\frac{(-x)^n}{\Gamma(\beta-n\alpha)}
    +O(x^{-N-1}).
\]

By Stirling's formula,
\[
\log\Gamma(z)=z\log z-z+\tfrac12\log z+O(1)\quad\text{as $|z|\to\infty$,}
\]
so as $n\to\infty$, and putting $\beta'=1-\beta$,
\begin{align*}
\smash[b]{\log\biggl(\frac{\Gamma(n\alpha+\beta')}%
{\Gamma\bigl((n-1)\alpha+\beta')}\biggr)}&=
\bigl(n\alpha+\beta'\bigr)\log(n\alpha+\beta')\\
&\qquad{}-\bigl((n-1)\alpha+\beta'\bigr)
    \log\bigl((n-1)\alpha+\beta'\bigr)+O(1)\\
    &=\alpha\log(n\alpha+\beta')+\bigl((n-1)\alpha+\beta'\bigr)
    \log\frac{n\alpha+\beta'}{(n-1)\alpha+\beta'}+O(1)\\
    &=\alpha\log\bigl(n(\alpha+O(n^{-1})\bigr)+\bigl((n-1)\alpha+\beta'\bigr)
    \log\frac{1+O(n^{-1})}{1+O(n^{-1})}+O(1)\\
    &=\alpha\log n+O(1)=\log n^\alpha+O(1).
\end{align*}
Thus,
\[
\log\frac{\Gamma(n\alpha+\beta')x^{-n}}%
{\Gamma\bigl((n-1)\alpha+\beta'\bigr)x^{-n+1}}=\log{n^\alpha}{x}+O(1),
\]
showing that the terms of the asymptotic expansion decrease in magnitude until 
$n^\alpha\approx x$.

For large~$n$, there is a risk that $\Gamma(n\alpha-\beta+1)$ overflows.
For example, in IEEE double precision arithmetic, $\Gamma(172)=\mathtt{Inf}$.
Standard numerical libraries therefore provide a $\log\Gamma(x)$ function 
which will not overflow unless $x$ itself is close to overflowing.  We 
therefore store
\[
a_n=(-1)^n\sin\pi(n\alpha-\beta)
\quad\text{and}\quad
b_n=\log\Gamma(n\alpha-\beta+1),
\]
and compute the terms of the asymptotic expansion as
\[
a_nb_nx^{-n}=a_n\exp(b_n-n\log x).
\]
(Notice that $a_1=\sin\pi(\beta-\alpha)>0$ if $0<\beta-\alpha<1$.)

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Asymptotic expansion: positive real argument}
Putting $z=x>0$ in~\eqref{eq: RN(z)} gives
\[
R_N(x)=\frac{-x^{-N-1}}{2\pi i}\int_{-\infty}^{0^+}
\frac{e^w w^{(N+1)\alpha-\beta}}{1-x^{-1}w^\alpha}\,dw,
\]
where the Hankel contour must be such that $x<|w^\alpha|$ for all~$w$.
Since we want to consider large~$x$, we move the contour to the left of the 
pole at~$w=x^{1/\alpha}$, picking up a residue
\[
\res_{w=x^{1/\alpha}}\frac{e^w w^{(N+1)\alpha-\beta}}{1-x^{-1}w^\alpha}
    =\lim_{w\to x^{1/\alpha}}e^w w^{(N+1)\alpha-\beta}\,
    \frac{w-x^{1/\alpha}}{1-x^{-1}w^\alpha}
    =\frac{-x^{N+1}}{\alpha}\,x^{(1-\beta)/\alpha}\exp(x^{1/\alpha}),
\]
leading to the expansion
\[
E_{\alpha,\beta}(x)=x^{(1-\beta)/\alpha}\exp(x^{1/\alpha})
    +\frac{1}{\pi}\sum_{n=1}^Na_nb_nx^{-n}+\tilde R_N(x).
\]







A deeper analysis of the exponential asymptotics of~$E_{\alpha,\beta}$ shows
that~\cite[Theorem~2.2]{WongZhao2002}
\[
E_{\alpha,\beta}(-x)=\frac{1}{\pi}\sum_{n=1}^{[x^{1/\alpha}]}
    A_nB_nx^{-n}+O\bigl(x^{1/2-\beta}e^{-x}\bigr)
\]
for $x>0$ and $0<\alpha\le 1-\epsilon$, where
\[
A_n=(-1)^{n+1}\sin\pi(n\alpha-\beta)
\quad\text{and}\quad
B_n=\Gamma(n\alpha-\beta+1).
\]
To reduce the risk of overflow for large~$n$, we store $\log B_n$ and compute
\[
B_nx^{-n}=\exp(\log B_n-n\log x).
\]
For optimal truncation, we choose $N$ so that if $x=N^\alpha$ (and hence
$N\approx x^{1/\alpha}$) then $B_Nx^{-N}=B_N/N^{N\alpha}<\epsilon$, or 
equivalently,
\[
N\alpha\log N-\log B_N>\log\epsilon^{-1}.
\]

Replacing $\beta$ with $\beta-r\alpha$ in the identity~\eqref{eq: beta identity}
gives
\[
z^rE_{\alpha,\beta}(z)=E_{\alpha,\beta-r\alpha}(z)-\sum_{n=0}^{r-1}
\frac{z^n}{\Gamma(\beta-r\alpha+n\alpha)},
\]
and so
\[
E_{\alpha,\beta}(z)=z^{-r}E_{\alpha,\beta-r\alpha}(z)-\sum_{n=0}^{r-1}
\frac{z^{-(r-n)}}{\Gamma\bigl(\beta-(r-n)\alpha\bigr)},
\]
leading to the asymptotic expansion
\[
E_{\alpha,\beta}(z)=-\sum_{n=1}^r\frac{z^{-n}}{\Gamma(\beta-n\alpha)}
    +O(z^{-r-1}).
\]
Recalling that the Gamma function satisfies
\[
\Gamma(z)\Gamma(1-z)=\frac{\pi}{\sin\pi z},
\]
it follows that
\[
\frac{-1}{\Gamma(\beta-n\alpha)}=\frac{1}{\pi}\,\sin\pi(n\alpha-\beta)
    \Gamma(1+n\alpha-\beta),
\]
so
\[
E_{\alpha,\beta}(z)=\frac{1}{\pi}\sum_{n=1}^r\sin\pi(n\alpha-\beta)
    \Gamma(1+n\alpha-\beta)z^{-n}+O(z^{-r-1}).
\]






%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Chebyshev expansion}
Consider the substitution
\[
y=\frac{x-1}{x+1}\quad\text{for $0\le x<\infty$.}
\]
The function~$x\mapsto y$ maps the half-line~$[0,\infty)$ onto~$[-1,1)$, and 
the inverse function~$y\mapsto x$ is given by
\[
x=\frac{1+y}{1-y}\quad\text{for $-1\le y<1$.}
\]
We define a function~$g$ by requiring that
\[
E_{\alpha,\beta}(-x)=\tfrac12(1-y)[1+(1+y)g(y)],
\]
or equivalently,
\[
g(y)=\frac{1}{1+y}\biggl(\frac{2E_{\alpha,\beta}(-x)}{1-y}-1\biggr).
\]
Notice that
\[
1+y=\frac{2x}{x+1}\quad\text{and}\quad 1-y=\frac{2}{x+1}.
\]

On the one hand, the power series~\eqref{eq: E alpha beta def} implies that
\[
g(y)=\frac{1}{2}\biggl(1-\frac{1}{\Gamma(\beta+\alpha)}\biggr)+O(x)
\quad\text{as $y\to-1$ (so $x\to0$),}
\]
and on the other hand, the asymptotic expansion~\eqref{eq: E minus asymptotic}
implies that
\[
g(y)=\frac{1}{2}\biggl(-1+\frac{1}{\pi}\,\sin\pi(\beta-\alpha)\,
    \Gamma(1+\alpha-\beta)\biggr)+O(x^{-1})\quad
\text{as $y\to1$ (so $x\to\infty$).}
\]
In particular, $g$ and all its derivatives are continuous on~$[-1,1]$.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Negative real argument when $1<\alpha<2$}
Assume for this section that $x>0$ and $1<\alpha<2$.  If follows that $F(w,x)$
has poles at $\gamma_\pm=x^{1/\alpha}e^{\pm i\pi/\alpha}$, 
with $\arg\gamma_\pm=\pm\pi/\alpha$ and hence
\[
\frac{\pi}{2}<\arg\gamma_+<\pi
\quad\text{and}\quad
-\pi<\arg\gamma_-<-\frac{\pi}{2}.
\]
Let
\[
\varphi(w,x)=\frac{(w-\gamma_+)(w-\gamma_-)}%
{(w^\alpha+x)(2w-\gamma_+-\gamma_-)}
\]
so that
\[
\frac{1}{w^\alpha+x}=\varphi(w;x)\biggl(
    \frac{1}{w-\gamma_+}+\frac{1}{w-\gamma_-}\biggr)
\]
with
\[
\varphi(\gamma_\pm;x)=\lim_{x\to\gamma_\pm}\varphi(w;x)
    =\frac{\gamma_\pm^{1-\alpha}}{\alpha}.
\]
Putting
\[
H_\pm(w,x)=\frac{w^{\alpha-\beta}\varphi(w;x)
-\gamma_\pm^{\alpha-\beta}\varphi(\gamma_\pm;x)}{w-\gamma_\pm}
    =\frac{w^{\alpha-\beta}(w-\gamma_\mp)}{(w^\alpha+x)(2w-\gamma_+-\gamma_-)}
-\frac{\alpha^{-1}\gamma_\pm^{1-\beta}}{w-\gamma_\pm},
\]
we have
\[
w^{\alpha-\beta}\,\frac{\varphi(w;x)}{w-\gamma_\pm}
    =\frac{\alpha^{-1}\gamma_\pm^{1-\beta}}{w-\gamma_\pm}+H_\pm(w,x)
\]
so
\[
F(w,x)=\frac{1}{\alpha}\biggl(
     \frac{\gamma_+^{1-\beta}}{w-\gamma_+}
    +\frac{\gamma_-^{1-\beta}}{w-\gamma_-}\biggr)
    +H_+(w;x)+H_-(w;x)
\]
and hence
\begin{align*}
E_{\alpha,\beta}(-x)&=\frac{1}{2\pi i}\int_{-\infty}^{0^+}e^wF(w,x)\,dw\\
&=\frac{1}{\alpha}\,\bigl(\gamma_+^{1-\beta}e^{\gamma_+}
    +\gamma_-^{1-\beta}e^{\gamma_-}\bigr)
     +\frac{1}{2\pi i}\int_{-\infty}^{0^+}e^w\bigl(H_+(w,x)+H_-(w,x)\bigr)\,dw.
\end{align*}
Since
\begin{align*}
\gamma_+^{1-\beta}e^\gamma_+&=x^{(1-\beta)/\alpha}
    \exp\bigl(i\pi(1-\beta)/\alpha\bigr)
    \exp\bigl(x^{1/\alpha}(\cos\pi/\alpha+i\sin\pi/\alpha)\bigr)\\
    &=x^{(1-\beta)/\alpha}\exp(x^{1/\alpha}\cos\pi/\alpha)
    \exp\bigl(i\pi(1-\beta)/\alpha+ix^{1/\alpha}\sin\pi/\alpha\bigr)
\end{align*}
it follows that
\[
\gamma_+^{1-\beta}e^{\gamma_+}
    +\gamma_-^{1-\beta}e^{\gamma_-}
    =2x^{(1-\beta)/\alpha}\,\exp(x^{1/\alpha}\cos\pi/\alpha)
    \cos\bigl(\pi(1-\beta)/\alpha+x^{1/\alpha}\sin\pi/\alpha\bigr).
\]

To evaluate $H_\pm(w;x)$ for~$w$ near~$\gamma_\pm$, let
\[
w=\gamma_\pm(1+\epsilon_\pm)\quad\text{where}\quad
\epsilon_\pm=\frac{w-\gamma_\pm}{\gamma_\pm}.
\]
so that
\[
w^\alpha+x=-x(1+\epsilon_\pm)^\alpha+x
    =-x\epsilon_\pm\psi_{1,\alpha}(\epsilon_\pm)
    =-x\epsilon_\pm[\alpha+\epsilon_\pm\psi_{2,\alpha}(\epsilon_\pm)]
\]
with
\begin{gather*}
w-\gamma_\pm=\gamma_\pm\epsilon_\pm,\qquad
2w-\gamma_+-\gamma_-=w-\gamma_\mp+\gamma_\pm\epsilon_\pm,\\
w^{\alpha-\beta}=-x\gamma_\pm^{-\beta}
    [1+\epsilon_\pm\psi_{1,\alpha-\beta}(\epsilon_\pm)].
\end{gather*}
We have
\[
H_\pm(w;x)=\frac{w^{\alpha-\beta}(w-\gamma_\mp)(w-\gamma_\pm)
-\alpha^{-1}\gamma_\pm^{1-\beta}(w^\alpha+x)(2w-\gamma_\pm-\gamma_\mp)}%
{(w^\alpha+x)(2w-\gamma_+-\gamma_-)(w-\gamma_\pm)},
\]
where the numerator equals
\begin{align*}
-x\gamma_\pm^{-\beta}&[1+\epsilon_\pm\psi_{1,\alpha-\beta}(\epsilon_\pm)]
(w-\gamma_\mp)(\gamma_\pm\epsilon_\pm)
+\alpha^{-1}\gamma_\pm^{1-\beta}
x\epsilon_\pm[\alpha+\epsilon_\pm\psi_{2,\alpha}(\epsilon_\pm)]
(w-\gamma_\mp+\gamma_\pm\epsilon_\pm)\\
&=x\gamma_\pm^{1-\beta}\epsilon_\pm\Bigl(
[1+\alpha^{-1}\epsilon_\pm\psi_{2,\alpha}(\epsilon_\pm)]
(w-\gamma_\mp+\gamma_\pm\epsilon_\pm)
-[1+\epsilon_\pm\psi_{1,\alpha-\beta}(\epsilon_\pm)](w-\gamma_\mp)
\Bigr)\\
&=x\gamma_\pm^{1-\beta}\epsilon_\pm^2\Bigl((w-\gamma_\mp)
[\alpha^{-1}\psi_{2,\alpha}(\epsilon_\pm)-\psi_{1,\alpha-\beta}(\epsilon_\pm)]
+\gamma_\pm[1+\alpha^{-1}\epsilon_\pm\psi_{2,\alpha}(\epsilon_\pm)]\Bigr)
\end{align*}
and the denominator equals
\[
-x\epsilon_\pm\psi_{1,\alpha}(\epsilon_\pm)
(w-\gamma_\mp+\gamma_\pm\epsilon_\pm)
(\gamma_\pm\epsilon_\pm)=-x\gamma_\pm\epsilon_\pm^2\psi_{1,\alpha}(\epsilon_\pm)
(w-\gamma_\mp+\gamma_\pm\epsilon_\pm)
\]
so
\[
H_\pm(w;x)=\frac{(w-\gamma_\mp)
[\psi_{1,\alpha-\beta}(\epsilon_\pm)-\alpha^{-1}\psi_{2,\alpha}(\epsilon_\pm)]
-\gamma_\pm\alpha^{-1}\psi_{1,\alpha}(\epsilon_\pm)}%
{\gamma_\pm^\beta\,\psi_{1,\alpha}
(\epsilon_\pm)(w-\gamma_\mp-\gamma_\pm\epsilon_\pm)}.
\]
In particular,
\[
H_\pm(\gamma_\pm;x)
=\frac{(1+\alpha-2\beta)(\gamma_\pm-\gamma_\mp)-2\gamma_\pm}%
{\alpha\gamma_\pm^\beta(\gamma_\pm-\gamma_\mp)}.
\]
Similarly,
\[
H_\mp(w;x)=\frac{w^{\alpha-\beta}(w-\gamma_\pm)(w-\gamma_\mp)
-\alpha^{-1}\gamma_\mp^{1-\beta}(w^\alpha+x)(2w-\gamma_\mp-\gamma_\pm)}%
{(w^\alpha+x)(2w-\gamma_+-\gamma_-)(w-\gamma_\mp)},
\]
where the numerator equals
\begin{align*}
\gamma_\pm^{\alpha-\beta}&(1+\epsilon_\pm)^{\alpha-\beta}
(\gamma_\pm\epsilon_\pm)
(\gamma_\pm-\gamma_\mp+\gamma_\pm\epsilon_\pm)
+\alpha^{-1}\gamma_\mp^{1-\beta}x\epsilon_\pm\psi_{1,\alpha}(\epsilon_\pm)
(\gamma_\pm-\gamma_\mp+2\gamma_\pm\epsilon_\pm)\\
&=-x\epsilon_\pm\Bigl(\gamma_\pm^{1-\beta}(1+\epsilon_\pm)^{\alpha-\beta}
(w-\gamma_\mp)
-\alpha^{-1}\gamma_\mp^{1-\beta}\psi_{1,\alpha}(\epsilon_\pm)
(2w-\gamma_+-\gamma_-)\Bigr)
\end{align*}
and the denominator equals
\[
-x\epsilon_\pm\psi_{1,\alpha}(\epsilon_\pm)
(2w-\gamma_+-\gamma_-)(w-\gamma_\mp)
\]
so
\begin{align*}
H_\mp(w;x)&=\frac{\gamma_\pm^{1-\beta}(1+\epsilon_\pm)^{\alpha-\beta} 
(w-\gamma_\mp)-\alpha^{-1}\gamma_\mp^{1-\beta}\psi_{1,\alpha}(\epsilon_\pm)
(2w-\gamma_+-\gamma_-)}%
{\psi_{1,\alpha}(\epsilon_\pm)(2w-\gamma_+-\gamma_-)(w-\gamma_\mp)}\\
&=\frac{\gamma_\pm^{1-\beta}(1+\epsilon_\pm)^{\alpha-\beta}}%
{\psi_{1,\alpha}(\epsilon_\pm)(2w-\gamma_+-\gamma_-)}
-\frac{\alpha^{-1}\gamma_\mp^{1-\beta}}{w-\gamma_\mp}.
\end{align*}
In particular,
\[
H_\mp(\gamma_\pm;x)=\frac{\gamma_\pm^{1-\beta}-\gamma_\mp^{1-\beta}}%
{\alpha(\gamma_\pm-\gamma_\mp)}=\frac{1}{x^{\beta/\alpha}}\,
\frac{\sin\pi(1-\beta)/\alpha}{\alpha\sin\pi/\alpha}.
\]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Parabolic contour}
Consider $\mathcal{C}$ of the form
\[
w(u)=\mu(iu+1)^2\quad\text{for $-\infty<u<\infty$,}
\]
with $\mu>0$.  It follows that
\begin{align*}
\Re w(u+iv)&=\mu\bigl((1-v)^2-u^2\bigr),\\
\Im w(u+iv)&=2\mu(1-v)u,
\end{align*}
and so $\Im w(u+iv)\to-\infty$ as~$|u|\to\infty$ provided $v<1$.  Thus, we can 
apply the error bound~\eqref{eq: Qh error} for $r=1-\epsilon$ and $s>0$.
Neglecting $\epsilon$, we deduce that the error is of order
\[
\exp(-2\pi/h)+\exp\bigl(\mu(1+s)^2-2\pi s/h\bigr)
    +\exp\bigl(\mu(1-(Nh)^2)\bigr).
\]
The exponent
\[
\mu(1+s)^2-\frac{2\pi s}{h}=\mu\bigg[\biggl(1+s-\frac{\pi}{\mu h}\biggr)^2
    +\frac{2\pi}{\mu h}-\biggl(\frac{\pi}{\mu h}\biggr)^2\biggr]
\]
is minimised if
\[
s=1-\frac{\pi}{\mu h},
\]
and using this value we balance the three error terms by satisfying
\[
\frac{-2\pi}{h}=\frac{2\pi}{h}-\frac{\pi^2}{\mu h^2}=\mu\bigl(1-(Nh)^2\bigr).
\]
The left-hand equation yields $\mu h=\pi/4$ so $1-(Nh)^2=-2\pi/(\mu h)=-8$
and hence $Nh=3$.  Thus, for a given~$N$, the optimal quadrature parameters are
\[
h_\star=\frac{3}{N}\quad\text{and}\quad\mu_\star=\frac{\pi}{12}\,N,
\]
yielding an error of order $\exp(-2\pi/h_\star)=\exp(-2\pi N/3)$ or
$\exp(2\pi/3)^N\approx8.12^N$.

Since $w'(u)=2\mu i(iu+1)$ we see that
\[
\frac{hw'(u)}{2\pi i}=\frac{\mu h}{\pi}(iu+1)=\frac{1}{4}(1+iu)
\]
and so
\begin{align*}
Q_{h,N}(x)&=\frac{1}{4}\sum_{n=-N}^Ne^{w_n}f(w_n;x)(1+inh)\\
    &=\frac{1}{2}\biggl(\tfrac12e^{w_0}f(w_0;x)+\sum_{n=1}^N\Re\bigl(
    e^{w_n}f(w_n;x)(1+inh)\bigr)\biggr).
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Carath\'eodory--Fejer method}
Let $D=\{\,z\in\mathbb{C}:|z|<1\,\}$ denote the open unit disk in the complex 
plane, so that its boundary~$\partial D=\{\,z\in\mathbb{C}:|z|=1\,\}$ is the 
unit circle.  Given a function~$F:[-1,1]\to\mathbb{R}$ and an integer~$M\ge0$, 
denote the partial Chebyshev expansion of~$F$ by
\[
F_M(x)=\sideset{}{'}\sum_{k=0}^Ma_kT_k(x)
    \equiv\tfrac12 a_0+\sum_{k=1}^Ma_kT_k(x)\quad\text{for 
$-1\le x\le 1$,}
\]
where $T_k$ denote the Chebyshev polynomial (of the first kind) with degree~$k$,
and
\[
a_k=\frac{2}{\pi}\int_{-1}^1\frac{F(x)T_k(x)}{\sqrt{1-x^2}}\,dx
\]
is the $k$th Chebyshev coefficient of~$F$.  The change of variable
\[
x=\Re z=\tfrac12(z+z^{-1})
\]
defines a bijection~$z\mapsto x$ from the unit circle~$\partial D$ onto the 
real interval~$[-1,1]$, with
\[
T_k(x)=\tfrac12(z^k+z^{-k}).
\]
Put $a_{-k}=a_k$ and define
\[
f_M(z)=\sum_{k=-M}^M a_kz^k,
\]
and given fixed integers $m\ge 0$~and $n\ge0$, let
\[
f^+(z)=\sum_{k=m-n+1}^M a_kz^k\quad\text{and}\quad
f^0(z)=\begin{cases}
    \sum_{k=n-m}^{m-n}a_kz^k,&\text{if $m\ge n$,}\\
    -\sum_{k=m-n+1}^{n-m-1}a_kz^k&\text{if $m<n$.}
       \end{cases}
\]
In this way,
\begin{equation}\label{eq: FM}
F_M(x)=\tfrac12f_M(z)=\tfrac12\bigl[f^+(z)+f^+(z^{-1}) +f^0(z)\bigr].
\end{equation}

We wish to approximate $F(x)$ by a rational function of type~$(m,n)$, that is,
by a function $R(x)=p(x)/q(z)$ where $p$~and $q$ are real polynomials of degree 
$m$~and $n$, respectively.  Let $V_{mn}$ denote the set of such rational 
functions, and let $\tilde V_{mn}$ denote the set of functions of the form
\[
\tilde r(z)=\sum_{k=-\infty}^m d_kz^k\bigg/\sum_{k=0}^n e_kz^k,
\]
where the coefficients $d_k$~and $e_k$ are real, the numerator converges to 
a bounded analytic function in~$|z|>1$ and the denominator has no zeros 
in~$D\cup\partial D$.

Construct a real, $(M-m+n)\times(M-m+n)$ symmetric Hankel matrix
\[
H=[H_{ij}]\quad\text{where}\quad
H_{ij}=\begin{cases}
    a_{m-n+i+j},&\text{if $i+j\le M-m+n$,}\\
    0,&\text{otherwise.}
\end{cases}
\]

\begin{example}
If $M=5$, $m=3$ and $n=4$, then
\[
H=\begin{bmatrix}
a_0&a_1&a_2&a_3&a_4&a_5\\
a_1&a_2&a_3&a_4&a_5&0\\
a_2&a_3&a_4&a_5&0  &0\\
a_3&a_4&a_5&0  &0  &0\\
a_4&a_5&0  &0  &0  &0\\
a_5&0  &0  &0  &0  &0
\end{bmatrix}.
\]
\end{example}

Since $H$ is real and symmetric, it has an eigenvalue decomposition
\[
H=U\Lambda U^\top,
\]
where $U=[\boldsymbol{u}_1\quad\boldsymbol{u}_2\quad\cdots
\quad\boldsymbol{u}_{M-m+n}]$ is orthogonal and 
$\Lambda=\diag(\lambda_1,\lambda_2,\ldots,
\lambda_{M-m+n})$.  We order the (real) eigenvalues by absolute magnitude:
\[
|\lambda_1|\ge|\lambda_2|\ge\cdots\ge|\lambda_{M-m+n}|.
\]
We denote the $(n+1)$st eigenvalue and eigenvector by
\[
\lambda=\lambda_{n+1}\quad\text{and}\quad
\boldsymbol{u}_{n+1}=[u_0,u_1,\ldots,u_{M-m+n-1}]^\top,
\]
and define
\begin{align*}
b(z)&=\lambda z^M\sum_{j=0}^{M-m+n-1}u_jz^j\bigg/
\sum_{j=0}^{M-m+n-1}u_{M-m+n-j}z^j\\
    &=\lambda z^M\,\frac{u_0+u_1z+\cdots+u_{M-m+n-1}z^{M-m+n-1}}%
{u_{M-m+n-1}+u_{M-m+n-2}z+\cdots+u_0z^{M_n-m+n-1}}
\end{align*}
and
\[
\upsilon(z)=\sum_{j=0}^{M-m+n-1}u_jz^j.
\]
Since
\[
\sum_{j=0}^{M-m+n-1}u_{M-m+n-j}z^j=\sum_{j=0}^{M-m+n-1}u_0z^{M-m+n-j}
    =M^{M-m+n}\upsilon(z^{-1})
\]
we see that
\[
b(z)=\lambda z^{m-n}\frac{\upsilon(z)}{\upsilon(z^{-1})}.
\]
Thus, $|b(z)|=|\lambda|$ for~$z\in\partial D$, and since $\lambda$ and the 
components of~$\boldsymbol{u}_{n+1}$ are real,
\begin{equation}\label{eq: b zbar}
\overline{\upsilon(z)}=\upsilon(\bar z)\quad\text{and}\quad
\overline{b(z)}=b(\bar z), 
\end{equation}
and in particular,
\[
\overline{\upsilon(z)}=\upsilon(z^{-1})\quad\text{and}\quad
\overline{b(z)}=b(z^{-1})\quad\text{for $z\in\partial D$.}
\]

\begin{theorem}
The function $\tilde r^*=f^+-b$ is the unique best approximation to~$f^+$
from $\tilde V_{mn}$ on~$\partial D$, that is, $\tilde r^*$ is the unique 
function in~$V_{mn}$ satisfying
\[
\|f^+-\tilde r^*\|_{L_\infty(\partial D)}=\min_{g\in\tilde V_{mn}}
\|f^+-g\|_{L_\infty(\partial D)}.
\]
Moreover, 
\[
\|f^+-\tilde r^*\|_{L_\infty(\partial D)}=|\lambda|,
\]
and if $|\lambda_n|>|\lambda|>|\lambda_{n+2}|$ then the winding number 
of~$b$ is $m+n+1$.  
\end{theorem}
\begin{proof}
See~\cite[Theorem~1]{TrefethenGutknecht1983}.
\end{proof}

Define
\[
\tilde R(x)=\tfrac12\bigl[\tilde r^*(z)+\tilde r^*(z^{-1})+f^0(z)\bigr],
\]
and observe that by~\eqref{eq: FM},
\begin{equation}\label{eq: Rtilde FM}
\tilde R(x)=\tfrac12\bigl[f_M(z)-b(z)-b(z^{-1})\bigr]
    =F_M(x)-\tfrac12[b(z)+b(z^{-1})],
\end{equation}
so
\[
F_M(x)-\tilde R(x)=\Re b(z).
\]
The theorem above implies that there are points
\[
1=x_0>x_1>\cdots>x_{m+n+1}=-1
\]
such that
\[
\|F_M-\tilde R\|_\infty=|\lambda|\quad\text{and}\quad
(F_M-\tilde R)(x_j)=(-1)^j\lambda\quad\text{for $0\le j\le m+n+1$.}
\]

Let
\[
B(x)=\tfrac12[b(z)+b(z^{-1})]=\sideset{}{'}\sum_{k=0}^\infty b_kT_k(x)
\]
and note that the Chebyshev coefficients
\[
b_k=\frac{2}{\pi}\int_{-1}^1\frac{B(x)T_k(x)}{\sqrt{1-x^2}}\,dx
    =\frac{2}{\pi}\int_0^\pi B(\cos\theta)\cos(k\theta)\,d\theta.
\]
Thus, by~\eqref{eq: Rtilde FM},
\[
\tilde R(x)=\sideset{}{'}\sum_{k=0}^\infty\tilde c_kT_k(x)
\quad\text{where}\quad\tilde c_k=a_k-b_k.
\]

Let us factorize the denominator of~$b(z)$,
\[
\sum_{k=0}^{M-m+n-1}u_{M-m+n-j}z^j=u_0\prod_{j=1}^{M-m+n}(z-\zeta_j),
\]
and number the zeros~$\zeta_j$ so that
\[
\begin{aligned}
|\zeta_j|&>1&&\text{for $1\le j\le J$,}\\
|\zeta_j|&<1&&\text{for $J+1\le j\le M-m+n$.}
\end{aligned}
\]
Define
\[
q(z)=\prod_{j=1}^J\frac{z-\zeta_j}{-\zeta_j},
\]
which satisfies $q(0)=1$, and
\[
Q(x)=\frac{q(z)q(z^{-1})}{q(i)q(-i)},
\]
which satisfies $Q(0)=1$ and $|Q(x)|>0$ for~$-1\le x\le 1$.  We therefore have 
a Chebyshev expansion,
\[
\frac{1}{Q(x)}=\sideset{}{'}\sum_{k=0}^{2J}\gamma_kT_k(x).
\]
Finally, we define
\[
P(x)=\sideset{}{'}\sum_{k=0}^m\beta_kT_k(x)
\]
where the Chebyshev coefficients satisfy
\[
\begin{bmatrix}
\gamma_0     &\gamma_1&\cdots&\gamma_m    &\cdots&\gamma_{2m-1}&\gamma_{2m}\\
\gamma_1     &\gamma_0&\cdots&\gamma_{m-1}&\cdots&\gamma_{2m-2}&\gamma_{2m-1}\\
\vdots       &\vdots  &\ddots&\vdots      &\ddots&\vdots       &\vdots\\
\gamma_m     &\gamma_{m-1}&\cdots&\gamma_0&\cdots&\gamma_{m-1} &\gamma_m\\
\vdots       &\vdots  &\ddots&\vdots      &\ddots&\vdots       &\vdots\\
\gamma_{2m-1}&\gamma_{2m-2}&\cdots&\gamma_{m-1}&\cdots&\gamma_0&\gamma_1\\
\gamma_{2m}  &\gamma_{2m-1}&\cdots&\gamma_m&\cdots&\gamma_1&\gamma_0
\end{bmatrix}
\begin{bmatrix}\beta_0\\ \beta_1\\ \vdots\\ \beta_m\\ \vdots\\ \beta_1\\ \beta_0
\end{bmatrix}
=\begin{bmatrix}\tilde c_m\\ \tilde c_{m-1}\\ \vdots\\ \tilde c_0\\ \vdots\\
\tilde c_{m-1}\\ \tilde c_m
 \end{bmatrix}.
\]
Here, the symmetric Toeplitz matrix is known to be positive definite.  Our 
rational approximation is then
\[
F(x)\approx R(x)=\frac{P(x)}{Q(x)}.
\]

If we are given a function $G:[0,\infty]\to\mathbb{R}$, then by putting
\[
F(x)=G(t)\quad\text{where}\quad t=\frac{1+x}{1-x},
\]
we can construct $R(x)$ as above and use
\[
G(t)\approx R(x)\quad\text{where}\quad x=\frac{t-1}{t+1}.
\]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Adaptive Antoulas--Anderson algorithm}
We work with the barycentric representation
\[
r(z)=\frac{n(z)}{d(z)}
\quad\text{where}\quad
n(z)=\sum_{j=1}^m\frac{w_jf_j}{z-z_j}
\quad\text{and}\quad
d(z)=\sum_{j=1}^m\frac{w_j}{z-z_j},
\]
for a set of distinct (real or complex) \emph{support points} $z_1$, $z_2$, 
\dots, $z_m$.  The \emph{data values} $f_1$, $f_2$, \dots, $f_m$ and 
\emph{weight} $w_1$, $w_2$, \dots, $w_m$ may also be real or complex.  The 
\emph{node polynomial} associated with the support points is
\[
\ell(z)=\prod_{j=1}^m(z-z_j),
\]
and if we define the polynomials of degree at most~$m-1$,
\[
p(z)=\ell(z)n(z)=\sum_{j=1}^mw_jf_j\prod_{\substack{k=1\\ k\ne j}}^m(z-z_k)
\quad\text{and}\quad 
q(z)=\ell(z)d(z)=\sum_{j=1}^mw_j\prod_{\substack{k=1\\ k\ne j}}^m(z-z_k),
\]
then
\[
r(z)=\frac{p(z)}{q(z)}.
\]

The AAA algorithm requires as input a large 
\emph{sample set}~$S=\{s_k\}_{k=1}^K\subseteq\mathbb{C}$.  In step~1,
we define the constant approximation
\[
r^{(1)}=\frac{1}{K}\sum_{k=1}^Kf(s_j)
\]
and choose $z_1\in S$ by requiring
\[
|f(z_1)-r^{(0)}|=\max_{z\in S}|f(z)-r^{(0)}|.
\]
Defining $f_1=f(z_1)$, $S^{(1)}=S\setminus\{z_1\}$ and the discrete 2-norm
\[
\|g\|_{S^{(1)}}=\sqrt{\sum_{k=1}^K|g(s_j)|^2},
\]
we choose
\[
w^{(1)}_1=\argmin_{|w|=1}
\bigl\|fd^{(1)}-n^{(1)}\bigr\|_{S^{(1)}},
\]
where
\[
n^{(1)}(z)=\frac{w^{(1)}_1f_1}{z-z_1}\quad\text{and}\quad
d^{(1)}(z)=\frac{w^{(1)}_1}{z-z_1}.
\]

For~$m\ge1$, at the start of the $m$th step we have already constructed
\[
r^{(m-1)}(z)=\frac{n^{(m-1)}(z)}{d^{(m-1)}(z)},\quad
n^{(m-1)}(z)=\sum_{j=1}^{m-1}\frac{w^{(m-1)}_jf_j}{z-z_j},\quad
d^{(m-1)}(z)=\sum_{j=1}^{m-1}\frac{w^{(m-1)}_j}{z-z_j}.
\]
We choose $z_m\in S^{(m-1)}=S\setminus\{z_1,z_2,\ldots,z_{m-1}\}$ by requiring
\[
\bigl|f(z_m)-r^{(m-1)}(z_m)\bigr|=\max_{z\in S^{(m-1)}}
    \bigl|f(z)-r^{(m-1)}(z)\bigr|,
\]
and put $f_m=f(z_m)$.  The vector of 
weights~$\boldsymbol{w}^{(m)}\in\mathbb{C}^m$ is then chosen as
\[
\boldsymbol{w}^{(m-1)}=\argmin_{\|w\|=1}\|fd^{(m)}-n^{(m)}\|_{S^{(m)}}.
\]
We stop the iteration when this norm falls below the desired error threshhold.

To solve the least-squares problem at the $m$th step, we enumerate the elements 
of the set~$S^{(m)}=\{s_{k_1},s_{k_2},\ldots,s_{k_{M-m}}\}$ and 
define the column vectors
\[
\boldsymbol{S}^{(m)}=\begin{bmatrix}s_{k_1}\\ s_{k_2}\\ \vdots\\
s_{k_{M-m}}\end{bmatrix}
\quad\text{and}\quad
\boldsymbol{F}^{(m)}=\begin{bmatrix}f(s_{k_1})\\ f(s_{k_2})\\ 
\vdots\\ f(s_{k_{M-m}})\end{bmatrix},
\]
so that
\[
f(s_{k_i})d^{(m)}(s_{k_i})-n^{(m)}(s_{k_i})
    =\sum_{j=1}^m\frac{F^{(m)}_i-f_j}{S^{(m)}_i-z_j}\,w_j
    =(A^{(m)}\boldsymbol{w})_i,
\]
where the $(M-m)\times m$ matrix~$A^{(m)}$ has entries
\[
(A^{(m)})_{ij}=\frac{F^{(m)}_i-f_j}{S^{(m)}_i-z_j}
\quad\text{for $1\le i\le M-m$ and $1\le j\le m$.}
\]
We compute the thin singular value decomposition $A^{(m)}=U\Sigma V^*$,
so $\Sigma=\diag(\sigma_1,\sigma_2,\ldots,\sigma_m)$ is $(M-m)\times m$ and 
\[
\|A^{(m)}\boldsymbol{w}\|^2=\|\Sigma V^*\boldsymbol{w}\|^2
    =\sum_{j=1}^m\bigl(\sigma_j\boldsymbol{v}_j^*\boldsymbol{w}\bigr)^2,
\]
where $\boldsymbol{v}_j$ is the $j$th column of~$V$.  Since
$\sigma_1\ge\sigma_2\ge\cdots\ge\sigma_m$, the norm is minimised choosing
$\boldsymbol{w}=\boldsymbol{v}_m$, in which case
\[
\|fd^{(m)}-n^{(m)}\|_{S^{(m)}}=\|A^{(m)}\boldsymbol{w}^{(m)}\|=\sigma_m.
\]






%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\printbibliography
\end{document}

